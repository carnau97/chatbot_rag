{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faeb271b",
   "metadata": {},
   "source": [
    "# Pinecone - Creando un asistente conversacional\n",
    "\n",
    "### Arquitectura RAG\n",
    "\n",
    "## IntroducciÃ³n <a name=\"intro\"></a>\n",
    "\n",
    "El propÃ³sito general de este notebook es generar un asistente conversacional basado en la arquitectura RAG (_Retrieval Augmented Generation_).\n",
    "\n",
    "1. Dispondremos de unos documentos PDFs que serÃ¡n nuestra base de conocimiento, los cuÃ¡les vectorizaremos y almacenaremos como Embeddings en un Ã­ndice de Pinecone\n",
    "2. Posteriormente, a travÃ©s de LangChain podremos lanzar queries y que automÃ¡ticamente se pasen por el modelo de embedding y se conecten al Ã­ndice de Pinecone para hacer una bÃºsqueda por similitud, para posteriormente pasarle los trozos mÃ¡s relevantes al LLM para que nos devuelva una respuesta.\n",
    "\n",
    "## LangChain ğŸ¦œ <a name=\"langchain\"></a>\n",
    "\n",
    "__LangChain__ es un marco para desarrollar aplicaciones basadas en modelos del lenguaje (Ãºnicamente se especializa en NLP)\n",
    "\n",
    "Para instalar LangChain en Python haremos:\n",
    "\n",
    "```python\n",
    "!pip install langchain\n",
    "```\n",
    "\n",
    "AdemÃ¡s de permitirnos encadenar conversaciones, LangChain tiene distintas funciones para leer archivos y hacer las divisiones de los textos en chunks\n",
    "\n",
    "El mÃ³dulo `document_loaders` https://python.langchain.com/docs/modules/data_connection/document_loaders/ tiene la capacidad de cargar los siguientes tipos de archivos:\n",
    "+ CSV\n",
    "+ Directorios\n",
    "+ PDF\n",
    "+ Markdown y texto\n",
    "+ HTML\n",
    "+ JSON\n",
    "\n",
    "Importamos un archivo PDF, previamente necesitamos instalar una dependencia\n",
    "```python\n",
    "!pip install pypdf\n",
    "```\n",
    "\n",
    "NOTA: Con la funciÃ³n `PyPDFDirectoryLoader` pueden cargarse todos los PDFs almancenados en un directorio, si se quiere cargar un Ãºnico documento puede emplearse la funciÃ³n `PyPDFLoader`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9bdc925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_core in /Applications/anaconda3/lib/python3.12/site-packages (0.0.13)\n",
      "Collecting langchain_core\n",
      "  Using cached langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Applications/anaconda3/lib/python3.12/site-packages (from langchain_core) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Applications/anaconda3/lib/python3.12/site-packages (from langchain_core) (1.33)\n",
      "Collecting langsmith<0.2.0,>=0.1.125 (from langchain_core)\n",
      "  Using cached langsmith-0.1.142-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Applications/anaconda3/lib/python3.12/site-packages (from langchain_core) (23.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Applications/anaconda3/lib/python3.12/site-packages (from langchain_core) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Applications/anaconda3/lib/python3.12/site-packages (from langchain_core) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Applications/anaconda3/lib/python3.12/site-packages (from langchain_core) (4.11.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Applications/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Applications/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain_core) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Applications/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain_core) (3.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in /Applications/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain_core) (2.27.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Applications/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain_core) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Applications/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Applications/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (2.20.1)\n",
      "Requirement already satisfied: anyio in /Applications/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (3.7.1)\n",
      "Requirement already satisfied: certifi in /Applications/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Applications/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (1.0.2)\n",
      "Requirement already satisfied: idna in /Applications/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (3.7)\n",
      "Requirement already satisfied: sniffio in /Applications/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Applications/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (0.14.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Applications/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain_core) (1.26.20)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Applications/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain_core) (2.0.12)\n",
      "Using cached langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
      "Using cached langsmith-0.1.142-py3-none-any.whl (306 kB)\n",
      "Installing collected packages: langsmith, langchain_core\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.87\n",
      "    Uninstalling langsmith-0.0.87:\n",
      "      Successfully uninstalled langsmith-0.0.87\n",
      "  Attempting uninstall: langchain_core\n",
      "    Found existing installation: langchain-core 0.0.13\n",
      "    Uninstalling langchain-core-0.0.13:\n",
      "      Successfully uninstalled langchain-core-0.0.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.0.13 requires langchain-core<0.2,>=0.1.9, but you have langchain-core 0.3.15 which is incompatible.\n",
      "langchain-community 0.0.13 requires langsmith<0.1.0,>=0.0.63, but you have langsmith 0.1.142 which is incompatible.\n",
      "langchain 0.0.345 requires langchain-core<0.1,>=0.0.9, but you have langchain-core 0.3.15 which is incompatible.\n",
      "langchain 0.0.345 requires langsmith<0.1.0,>=0.0.63, but you have langsmith 0.1.142 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain_core-0.3.15 langsmith-0.1.142\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba16ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%pip install --upgrade langchain-community\n",
    "#%pip install --upgrade pypdf\n",
    "##%pip install --upgrade sentence_transformers\n",
    "#%pip install --upgrade langchain_pinecone\n",
    "#%pip install --upgrade langchain langchain_core\n",
    "#%pip install --upgrade langchain\n",
    "#%pip install --upgrade huggingface-hub\n",
    "#%pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d8637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-community)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<2.0.36,>=1.4 (from langchain-community)\n",
      "  Downloading SQLAlchemy-2.0.35-cp310-cp310-macosx_10_9_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.10.10-cp310-cp310-macosx_10_9_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting langchain<0.4.0,>=0.3.6 (from langchain-community)\n",
      "  Using cached langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain-community)\n",
      "  Using cached langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-community)\n",
      "  Using cached langsmith-0.1.142-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain-community)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Using cached pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting requests<3,>=2 (from langchain-community)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain-community)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.17.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (64 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.6->langchain-community)\n",
      "  Using cached langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain<0.4.0,>=0.3.6->langchain-community)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.15->langchain-community)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (4.12.2)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.125->langchain-community)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-community)\n",
      "  Downloading orjson-3.10.11-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (50 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-community)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain-community)\n",
      "  Downloading charset_normalizer-3.4.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain-community)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain-community)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain-community)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<2.0.36,>=1.4->langchain-community)\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community)\n",
      "  Downloading anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting sniffio (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-community)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community)\n",
      "  Downloading pydantic_core-2.23.4-cp310-cp310-macosx_10_12_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.2.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
      "Using cached langchain_community-0.3.5-py3-none-any.whl (2.4 MB)\n",
      "Downloading aiohttp-3.10.10-cp310-cp310-macosx_10_9_x86_64.whl (399 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached langchain-0.3.7-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
      "Using cached langsmith-0.1.142-py3-none-any.whl (306 kB)\n",
      "Downloading numpy-1.26.4-cp310-cp310-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-macosx_10_9_x86_64.whl (184 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading SQLAlchemy-2.0.35-cp310-cp310-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp310-cp310-macosx_10_9_x86_64.whl (125 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-macosx_10_9_x86_64.whl (54 kB)\n",
      "Downloading greenlet-3.1.1-cp310-cp310-macosx_11_0_universal2.whl (271 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
      "Using cached marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-macosx_10_9_x86_64.whl (29 kB)\n",
      "Downloading orjson-3.10.11-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (266 kB)\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp310-cp310-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading yarl-1.17.1-cp310-cp310-macosx_10_9_x86_64.whl (93 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading propcache-0.2.0-cp310-cp310-macosx_10_9_x86_64.whl (46 kB)\n",
      "Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: urllib3, tenacity, sniffio, PyYAML, python-dotenv, pydantic-core, propcache, orjson, numpy, mypy-extensions, multidict, marshmallow, jsonpointer, idna, httpx-sse, h11, greenlet, frozenlist, charset-normalizer, certifi, attrs, async-timeout, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, pydantic, jsonpatch, httpcore, anyio, aiosignal, requests-toolbelt, pydantic-settings, httpx, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.2.post1 async-timeout-4.0.3 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.4.0 dataclasses-json-0.6.7 frozenlist-1.5.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 httpx-sse-0.4.0 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.7 langchain-community-0.3.5 langchain-core-0.3.15 langchain-text-splitters-0.3.2 langsmith-0.1.142 marshmallow-3.23.1 multidict-6.1.0 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.11 propcache-0.2.0 pydantic-2.9.2 pydantic-core-2.23.4 pydantic-settings-2.6.1 python-dotenv-1.0.1 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.0.0 typing-inspect-0.9.0 urllib3-2.2.3 yarl-1.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pypdf\n",
      "  Using cached pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in ./.conda/lib/python3.10/site-packages (from pypdf) (4.12.2)\n",
      "Using cached pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tqdm (from sentence_transformers)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Downloading torch-2.2.2-cp310-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-macosx_10_9_x86_64.whl.metadata (13 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-macosx_14_0_x86_64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence_transformers)\n",
      "  Downloading pillow-11.0.0-cp310-cp310-macosx_10_10_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Collecting sympy (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence_transformers)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.11.0->sentence_transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading torch-2.2.2-cp310-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "Downloading pillow-11.0.0-cp310-cp310-macosx_10_10_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-macosx_14_0_x86_64.whl (25.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m25.5/25.5 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-macosx_10_9_x86_64.whl (287 kB)\n",
      "Downloading safetensors-0.4.5-cp310-cp310-macosx_10_12_x86_64.whl (392 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.20.3-cp310-cp310-macosx_10_12_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp310-cp310-macosx_10_9_universal2.whl (14 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, tqdm, threadpoolctl, sympy, scipy, safetensors, regex, Pillow, networkx, MarkupSafe, joblib, fsspec, filelock, scikit-learn, jinja2, huggingface-hub, torch, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.0.0 filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.2 jinja2-3.1.4 joblib-1.4.2 mpmath-1.3.0 networkx-3.4.2 regex-2024.11.6 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentence_transformers-3.2.1 sympy-1.13.3 threadpoolctl-3.5.0 tokenizers-0.20.3 torch-2.2.2 tqdm-4.67.0 transformers-4.46.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain_pinecone\n",
      "  Using cached langchain_pinecone-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting aiohttp<3.10,>=3.9.5 (from langchain_pinecone)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-macosx_10_9_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3 in ./.conda/lib/python3.10/site-packages (from langchain_pinecone) (0.3.15)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.conda/lib/python3.10/site-packages (from langchain_pinecone) (1.26.4)\n",
      "Collecting pinecone-client<6.0.0,>=5.0.0 (from langchain_pinecone)\n",
      "  Using cached pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.10/site-packages (from aiohttp<3.10,>=3.9.5->langchain_pinecone) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.10/site-packages (from aiohttp<3.10,>=3.9.5->langchain_pinecone) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.10/site-packages (from aiohttp<3.10,>=3.9.5->langchain_pinecone) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.10/site-packages (from aiohttp<3.10,>=3.9.5->langchain_pinecone) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.10/site-packages (from aiohttp<3.10,>=3.9.5->langchain_pinecone) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.conda/lib/python3.10/site-packages (from aiohttp<3.10,>=3.9.5->langchain_pinecone) (4.0.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (0.1.142)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_pinecone) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in ./.conda/lib/python3.10/site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain_pinecone) (2024.8.30)\n",
      "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client<6.0.0,>=5.0.0->langchain_pinecone)\n",
      "  Using cached pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client<6.0.0,>=5.0.0->langchain_pinecone)\n",
      "  Using cached pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in ./.conda/lib/python3.10/site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain_pinecone) (4.67.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in ./.conda/lib/python3.10/site-packages (from pinecone-client<6.0.0,>=5.0.0->langchain_pinecone) (2.2.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain_pinecone) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (3.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain_pinecone) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain_pinecone) (2.23.4)\n",
      "Requirement already satisfied: idna>=2.0 in ./.conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp<3.10,>=3.9.5->langchain_pinecone) (3.10)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp<3.10,>=3.9.5->langchain_pinecone) (0.2.0)\n",
      "Requirement already satisfied: anyio in ./.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (1.0.6)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (3.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_pinecone) (1.2.2)\n",
      "Using cached langchain_pinecone-0.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-macosx_10_9_x86_64.whl (400 kB)\n",
      "Using cached pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
      "Using cached pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
      "Using cached pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client, aiohttp, langchain_pinecone\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.10.10\n",
      "    Uninstalling aiohttp-3.10.10:\n",
      "      Successfully uninstalled aiohttp-3.10.10\n",
      "Successfully installed aiohttp-3.9.5 langchain_pinecone-0.2.0 pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in ./.conda/lib/python3.10/site-packages (0.3.7)\n",
      "Requirement already satisfied: langchain_core in ./.conda/lib/python3.10/site-packages (0.3.15)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.10/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/lib/python3.10/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in ./.conda/lib/python3.10/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.conda/lib/python3.10/site-packages (from langchain) (0.1.142)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.conda/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.conda/lib/python3.10/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.conda/lib/python3.10/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.10/site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.10/site-packages (from langchain_core) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/lib/python3.10/site-packages (from langchain_core) (4.12.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in ./.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in ./.conda/lib/python3.10/site-packages (0.3.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.10/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/lib/python3.10/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in ./.conda/lib/python3.10/site-packages (from langchain) (0.3.15)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in ./.conda/lib/python3.10/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.conda/lib/python3.10/site-packages (from langchain) (0.1.142)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.conda/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.conda/lib/python3.10/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.conda/lib/python3.10/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in ./.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: huggingface-hub in ./.conda/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.10/site-packages (from huggingface-hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.conda/lib/python3.10/site-packages (from huggingface-hub) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.conda/lib/python3.10/site-packages (from huggingface-hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.10/site-packages (from huggingface-hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.10/site-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.conda/lib/python3.10/site-packages (from huggingface-hub) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.10/site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests->huggingface-hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests->huggingface-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests->huggingface-hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests->huggingface-hub) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain_openai\n",
      "  Using cached langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in ./.conda/lib/python3.10/site-packages (from langchain_openai) (0.3.15)\n",
      "Collecting openai<2.0.0,>=1.54.0 (from langchain_openai)\n",
      "  Using cached openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (0.1.142)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (4.6.2.post1)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.54.0->langchain_openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.54.0->langchain_openai)\n",
      "  Downloading jiter-0.7.0-cp310-cp310-macosx_10_12_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (4.67.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.conda/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.conda/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain_openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain_openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain_openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain_openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_openai) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.3)\n",
      "Using cached langchain_openai-0.2.6-py3-none-any.whl (50 kB)\n",
      "Using cached openai-1.54.3-py3-none-any.whl (389 kB)\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.7.0-cp310-cp310-macosx_10_12_x86_64.whl (292 kB)\n",
      "Installing collected packages: jiter, distro, tiktoken, openai, langchain_openai\n",
      "Successfully installed distro-1.9.0 jiter-0.7.0 langchain_openai-0.2.6 openai-1.54.3 tiktoken-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install langchain-community\n",
    "%pip install pypdf\n",
    "%pip install sentence_transformers\n",
    "%pip install langchain_pinecone\n",
    "%pip install langchain langchain_core\n",
    "%pip install langchain\n",
    "%pip install huggingface-hub\n",
    "%pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e07b2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b90e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_docs = \"/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2e1590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo/MASTER_INDEX.pdf', 'page': 0}, page_content='Contenido \\nMÃ¡ster en Big Data ................................ ................................ ...........................  3 \\nMÃ“DULO 1 - Fundamentos de tratamiento de datos para Data Science ............ 3 \\nMÃ“DULO 2 - Business intelligence ................................ ................................ . 5 \\nMÃ“DULO 3 - Aprendizaje AutomÃ¡tico Aplicado (Machine Learning) .................. 8 \\nMÃ“DULO 4 - MinerÃ­a de Texto y Procesamiento del Lenguaje Natural (PLN) ..... 10 \\nMÃ“DULO 5 - Inteligencia de Negocio y VisualizaciÃ³n ................................ ..... 12 \\nMÃ“DULO 6 - Infraestructura Big Data ................................ ...........................  15 \\nMÃ“DULO 7 - Almacenamiento e IntegraciÃ³n de Datos ................................ ... 18 \\nMÃ“DULO 8 - Valor y Contexto de la AnalÃ­tica Big Data ................................ ... 20 \\nMÃ“DULO 9 - Aplicaciones AnalÃ­ticas. Casos prÃ¡cticos ................................ .. 23 \\nMÃ“DULO 10 - Trabajo Fin de MÃ¡ster en Big Data ................................ ............ 24 \\nMÃ¡ster en Inteligencia Artificial y Deep Learning ................................ ............... 25 \\nMÃ“DULO 1 - Las herramientas del cientÃ­fico de datos ................................ ... 25 \\nMÃ“DULO 2 - Impacto y valor del big data ................................ ...................... 27 \\nMÃ“DULO 3 - Inteligencia artificial para la empresa ................................ ........ 29 \\nMÃ“DULO 4 - TecnologÃ­as y herramientas big data................................ .......... 32 \\nMÃ“DULO 5 - El Big Data en la empresa ................................ .........................  34 \\nMÃ“DULO 6 - Aplicaciones por sectores. Masterclasses, estudio de casos y \\ntalleres prÃ¡cticos ................................ ................................ ........................ 35 \\nMÃ“DULO 7 - Cloud, MLops, productivizaciÃ³n de modelos. IntroducciÃ³n a \\nprocess mining ................................ ................................ ...........................  36 \\nMÃ“DULO 8 - Series temporales y modelos prescriptivos. OptimizaciÃ³n. Modelos \\nde grafos ................................ ................................ ................................ .... 38 \\nMÃ“DULO 9 - Deep learning aplicada: NLP y visiÃ³n artificial ............................  40 \\nMÃ“DULO 10 - Trabajo Fin de MÃ¡ster en IA ................................ ..................... 42 \\nMÃ¡ster en Data Science ................................ ................................ .................. 43 \\nMÃ“DULO 1 - Las herramientas del cientÃ­fico de datos ................................ ... 43 \\nMÃ“DULO 2 - La ciencia de datos. TÃ©cnicas de anÃ¡lisis, minerÃ­a y visualizaciÃ³n 45 \\nMÃ“DULO 3 - EstadÃ­stica para cientÃ­ficos de datos ................................ ......... 47 \\nMÃ“DULO 4 - Aprendizaje automÃ¡tico ................................ ...........................  49 \\nMÃ“DULO 5 - Inteligencia artificial para la empresa ................................ ........ 51 '),\n",
       " Document(metadata={'source': '/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo/MASTER_INDEX.pdf', 'page': 1}, page_content='MÃ“DULO 6 - TecnologÃ­as y herramientas big data................................ .......... 53 \\nMÃ“DULO 7 - El trabajo del cientÃ­fico de datos: pasos y tÃ©cnicas en el anÃ¡lisis. \\nStorytelling ................................ ................................ ................................ . 55 \\nMÃ“DULO 8 - El proceso de aprendizaje automÃ¡tico: quÃ© es y quÃ© no es. DÃ³nde \\naplicar la inteligencia artificial ................................ ................................ ..... 56 \\nMÃ“DULO 9 - Nuevas tendencias: process mining, MLOps, cloud ................... 57 \\nMÃ“DULO 10 - Trabajo de Fin de Master en Data Science ................................  58 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n '),\n",
       " Document(metadata={'source': '/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo/MASTER_INDEX.pdf', 'page': 2}, page_content='MÃ¡ster en Big Data \\nURL: https://www.imf-formacion.com/masters-profesionales/master-big-data-\\nbusiness-intelligence  \\nMÃ“DULO 1 - Fundamentos de tratamiento de datos para Data \\nScience \\n1. Uso de mÃ¡quinas virtuales y shell de comandos \\no Concepto de mÃ¡quina virtual box \\no CreaciÃ³n y configuraciÃ³n de una mÃ¡quina virtual \\no Carga de una mÃ¡quina virtual \\no La shell de comandos de linucreaciÃ³n de scripts \\nUna mÃ¡quina virtual (VM) es un entorno de software que simula el hardware de una \\ncomputadora fÃ­sica, lo que permite ejecutar sistemas operativos y aplicaciones como si se \\ntratara de una mÃ¡quina real. VirtualBox es una herramienta popular que permite crear y \\ngestionar mÃ¡quinas virtuales, facilitando la ejecuciÃ³n de varios sistemas operativos en un \\nsolo equipo. La creaciÃ³n y configuraciÃ³n de una mÃ¡quina virtual implica definir \\nparÃ¡metros como la cantidad de memoria, el tipo de procesador y la capacidad del disco. \\nUna vez configurada, la mÃ¡quina se puede cargar para realizar tareas como si fuera una \\ncomputadora independiente. La shell de comandos de Linux es un potente entorno para \\nejecutar comandos y crear scripts que permiten automatizar procesos, realizar \\nconfiguraciones y gestionar el sistema de manera eficiente. \\n2. Fundamentos de programaciÃ³n en Python \\no El lenguaje Python y el entorno Jupyter notebook \\no Elementos bÃ¡sicos de Python \\no Estructuras de control \\no Estructuras de datos \\no Funciones \\no Excepciones \\no ImportaciÃ³n de mÃ³dulos \\no GestiÃ³n de ficheros \\nPython es un lenguaje de programaciÃ³n sencillo y versÃ¡til, ampliamente utilizado tanto \\nen desarrollo de software como en ciencia de datos. El entorno Jupyter Notebook \\nproporciona un espacio interactivo que facilita la programaciÃ³n en Python, permitiendo a \\nlos usuarios combinar cÃ³digo con texto y visualizaciones. Los elementos bÃ¡sicos de '),\n",
       " Document(metadata={'source': '/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo/MASTER_INDEX.pdf', 'page': 3}, page_content='Python incluyen variables, tipos de datos y operaciones matemÃ¡ticas. Las estructuras de \\ncontrol, como bucles y condicionales, permiten dirigir el flujo del programa. Python \\ntambiÃ©n cuenta con estructuras de datos como listas, diccionarios y tuplas que son \\nfundamentales para organizar la informaciÃ³n. AdemÃ¡s, las funciones permiten dividir el \\ncÃ³digo en bloques reutilizables, mientras que las excepciones ayudan a gestionar errores. \\nPython facilita la importaciÃ³n de mÃ³dulos que amplÃ­an su funcionalidad y ofrece  \\nherramientas para la gestiÃ³n de ficheros, lo cual es crucial para la manipulaciÃ³n de datos. \\n \\n3. Fundamentos de bases de datos relacionales \\no El modelo relacional \\no SQLite Studio \\no El lenguaje SQL \\nEl modelo relacional es la base de la mayorÃ­a de las bases de datos modernas, \\npermitiendo organizar la informaciÃ³n en tablas interconectadas por relaciones. \\nEste enfoque facilita la consulta y manipulaciÃ³n de datos de manera estructurada. \\nSQLite Studio es una herramienta que permite trabajar con bases de datos SQLite \\nde forma sencilla e intuitiva. El lenguaje SQL (Structured Query Language) es el \\nprincipal medio para interactuar con bases de datos relacionales, proporcionando \\ncomandos para realizar tareas como insertar, actualizar, eliminar y consultar datos. \\nA travÃ©s de SQL, se pueden gestionar grandes volÃºmenes de datos de forma \\neficiente y segura. \\n4. Fundamentos de tecnologÃ­as de internet \\no Formatos de almacenamiento de datos en internet \\no ManipulaciÃ³n de documentos CSV \\no ManipulaciÃ³n de documentos JSON \\no ManipulaciÃ³n de documentos XML \\nEn internet, los datos pueden ser almacenados y compartidos en varios formatos, \\ncomo CSV , JSON y XML. Los archivos CSV (Comma Separated Values) se utilizan \\npara almacenar datos tabulares de forma sencilla y fÃ¡cil de manipular. Los \\ndocumentos JSON (JavaScri pt Object Notation) son comunes para el intercambio \\nde informaciÃ³n entre aplicaciones debido a su estructura clara y ligera. XML \\n(eXtensible Markup Language), aunque menos utilizado que JSON hoy en dÃ­a, sigue \\nsiendo relevante para ciertos tipos de aplicaci ones y servicios web. La \\nmanipulaciÃ³n de estos formatos permite a los desarrolladores procesar y \\naprovechar los datos provenientes de diversas fuentes en la web. \\n5. Compartir datos, cÃ³digo y recursos en repositorios '),\n",
       " Document(metadata={'source': '/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo/MASTER_INDEX.pdf', 'page': 4}, page_content='o Repositorios digitales para compartir \\no La tecnologÃ­a GITHUB \\no Uso de Google Drive como repositorio digital \\nLos repositorios digitales permiten compartir cÃ³digo, datos y otros recursos de \\nmanera eficiente entre desarrolladores y colaboradores. GitHub es una plataforma \\nlÃ­der para almacenar y gestionar proyectos de software, facilitando la colaboraciÃ³n \\na travÃ©s de l control de versiones y la gestiÃ³n de cambios. Los usuarios pueden \\ntrabajar juntos en proyectos, realizar revisiones y compartir sus avances de manera \\npÃºblica o privada. Google Drive tambiÃ©n puede ser utilizado como un repositorio \\ndigital, especialmente Ãºtil para compartir documentos y archivos de forma sencilla \\ny accesible para cualquier colaborador. \\n6. Fundamentos de tratamiento de datos con el stack cientÃ­fico de Python \\no GestiÃ³n de matrices y cÃ¡lculo estadÃ­stico con NUMPY \\no RepresentaciÃ³n grÃ¡fica con MATPLOTLIB \\no ManipulaciÃ³n y anÃ¡lisis de datos con PANDAS \\nPython cuenta con un potente ecosistema de herramientas para el tratamiento y \\nanÃ¡lisis de datos, conocido como el stack cientÃ­fico. NumPy es una librerÃ­a \\nfundamental para la gestiÃ³n de matrices y la realizaciÃ³n de cÃ¡lculos estadÃ­sticos, \\nofreciendo funcione s de gran rendimiento para la manipulaciÃ³n de datos \\nnumÃ©ricos. Matplotlib es la librerÃ­a mÃ¡s popular para la representaciÃ³n grÃ¡fica en \\nPython, permitiendo crear visualizaciones que facilitan el entendimiento de los \\ndatos. Pandas, por otro lado, es esencial  para la manipulaciÃ³n y anÃ¡lisis de datos \\nestructurados, facilitando la limpieza y transformaciÃ³n de grandes volÃºmenes de \\ninformaciÃ³n. Estas herramientas en conjunto permiten a los usuarios realizar desde \\nanÃ¡lisis simples hasta tareas complejas de ciencia de datos. \\nMÃ“DULO 2 - Business intelligence \\n1. IntroducciÃ³n a la inteligencia de negocio \\no QuÃ© es la inteligencia de negocio \\no Importancia de los sistemas de inteligencia de negocio \\no Componentes de los sistemas de BI: arquitectura de inteligencia de \\nnegocio \\no Tipos de anÃ¡lisis que se pueden realizar \\no Inteligencia de negocio y analÃ­tica de negocio: BI y BA '),\n",
       " Document(metadata={'source': '/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo/MASTER_INDEX.pdf', 'page': 5}, page_content='o Inteligencia de negocio para Big Data \\nLa inteligencia de negocio (BI, por sus siglas en inglÃ©s) se refiere al uso de \\ntecnologÃ­as y estrategias para analizar datos y convertirlos en informaciÃ³n \\naccionable que apoye la toma de decisiones empresariales. Los sistemas de BI son \\nfundamentales para m ejorar la eficiencia operativa y la competitividad de las \\norganizaciones. Los componentes de un sistema de BI incluyen una arquitectura \\nque abarca fuentes de datos, almacenamiento, herramientas de anÃ¡lisis y \\nvisualizaciÃ³n. Los anÃ¡lisis que se pueden realiz ar incluyen anÃ¡lisis descriptivo, \\npredictivo y prescriptivo. AdemÃ¡s, existe una diferencia importante entre \\ninteligencia de negocio (BI) y analÃ­tica de negocio (BA), siendo BI mÃ¡s descriptivo y \\nBA mÃ¡s predictivo. BI tambiÃ©n desempeÃ±a un papel crucial en el  manejo de Big \\nData, ayudando a procesar grandes volÃºmenes de datos y proporcionando insights \\nestratÃ©gicos. \\n2. Almacenes de datos y bases de datos analÃ­ticas \\no Almacenes de datos \\no Herramientas de anÃ¡lisis de un almacÃ©n de datos: OLAP \\no Multidimensionalidad y el modelo multidimensional \\no DesnormalizaciÃ³n \\no Lenguajes de consulta analÃ­sticos: MDX \\nLos almacenes de datos (Data Warehouses) son sistemas diseÃ±ados para \\nalmacenar grandes volÃºmenes de informaciÃ³n de manera organizada y facilitar el \\nanÃ¡lisis de los datos histÃ³ricos. Las herramientas OLAP (Online Analytical \\nProcessing) permiten explorar los  datos almacenados desde mÃºltiples \\nperspectivas y realizar consultas complejas para identificar patrones y tendencias. \\nEl modelo multidimensional es clave en el anÃ¡lisis OLAP , ya que permite organizar \\nla informaciÃ³n en dimensiones que representan diferente s aspectos del negocio. \\nLa desnormalizaciÃ³n es una tÃ©cnica utilizada en almacenes de datos para optimizar \\nla velocidad de consulta. El lenguaje MDX (Multidimensional Expressions) es el \\nlenguaje de consulta empleado para trabajar con datos multidimensionales. \\n3. Herramientas de extracciÃ³n y carga \\no QuÃ© es el proceso de extracciÃ³n, transformaciÃ³n y carga (ETL) \\no Proceso ETL en un proyecto de inteligencia de negocio \\no Tipos de cargas \\no Gobierno del dato y orquestaciÃ³n '),\n",
       " Document(metadata={'source': '/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo/MASTER_INDEX.pdf', 'page': 6}, page_content='o Buenas prÃ¡cticas \\no Herramientas ETL: Pentaho Data Integration \\nEl proceso de ExtracciÃ³n, TransformaciÃ³n y Carga (ETL) es esencial en los proyectos \\nde inteligencia de negocio, ya que permite extraer datos de diferentes fuentes, \\ntransformarlos para adecuarlos a los estÃ¡ndares y cargarlos en un almacÃ©n de \\ndatos. Las carg as pueden ser completas o incrementales, dependiendo de las \\nnecesidades del proyecto. El gobierno del dato y la orquestaciÃ³n son aspectos \\nclave para asegurar la calidad y disponibilidad de los datos durante todo el proceso \\nETL. Pentaho Data Integration es una de las herramientas ETL mÃ¡s conocidas, \\npermitiendo llevar a cabo procesos de integraciÃ³n de datos de manera eficiente. \\nAdemÃ¡s, se recomienda seguir buenas prÃ¡cticas para asegurar la integridad y \\ncalidad de los datos en cada etapa del proceso. \\n4. Aplicaciones de inteligencia de negocio \\no Aplicaciones de inteligencia de negocio \\no Herramientas de inteligencia de negocio \\no Herramienta de inteligencia de negocio: Pentaho Business Analytics \\nLas aplicaciones de inteligencia de negocio permiten transformar grandes \\nvolÃºmenes de datos en informaciÃ³n comprensible para apoyar la toma de \\ndecisiones. Existen diversas herramientas de BI que permiten realizar anÃ¡lisis \\ndetallados y visualizaciones claras. Pentaho Business Analytics es una herramienta \\npoderosa que ofrece un conjunto completo de funciones para anÃ¡lisis de datos, \\ndesde la integraciÃ³n hasta la visualizaciÃ³n, facilitando la interpretaciÃ³n de los \\nresultados y el soporte a las decisiones empresariales. \\n5. AnÃ¡lisis de datos masivos aplicados al negocio \\no Datos externos \\no DEMO \\nEl anÃ¡lisis de datos masivos o Big Data permite a las empresas tomar decisiones \\ninformadas al evaluar tanto datos internos como externos. Los datos externos, \\ncomo redes sociales, informaciÃ³n del mercado y fuentes pÃºblicas, complementan \\nlos datos internos para proporcionar una visiÃ³n mÃ¡s completa . Este anÃ¡lisis se \\npuede demostrar mediante aplicaciones especÃ­ficas que muestran cÃ³mo los datos \\npueden impactar directamente en las estrategias empresariales. \\n6. Inteligencia de cliente (CRM) \\no CRM '),\n",
       " Document(metadata={'source': '/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo/MASTER_INDEX.pdf', 'page': 7}, page_content='o Inteligencia de cliente \\no Ingesta de datos CRM \\nLos sistemas CRM (Customer Relationship Management) son esenciales para \\ngestionar las relaciones con los clientes y analizar sus comportamientos y \\npreferencias. La inteligencia de cliente se refiere al anÃ¡lisis profundo de estos datos \\npara entender mejor a  los clientes y ofrecerles experiencias personalizadas. La \\ningesta de datos CRM implica la recopilaciÃ³n de informaciÃ³n de mÃºltiples fuentes \\npara alimentar el sistema y generar estrategias de marketing mÃ¡s efectivas y \\nfocalizadas. \\nMÃ“DULO 3 - Aprendizaje AutomÃ¡tico Aplicado (Machine Learning) \\n1. IntroducciÃ³n al aprendizaje automÃ¡tico \\no El proceso de la minerÃ­a de datos \\no Tipos de aprendizaje automÃ¡tico \\no IntroducciÃ³n a SCIKIT-LEARN y THEANO \\no Uso bÃ¡sico de un modelo \\nEl aprendizaje automÃ¡tico es una rama de la inteligencia artificial que se centra en \\nel desarrollo de algoritmos que permiten a las computadoras aprender de los datos \\ny hacer predicciones o tomar decisiones sin ser programadas explÃ­citamente para \\ncada tarea. El proceso de la minerÃ­a de datos consiste en extraer patrones Ãºtiles y \\nconocimiento de grandes volÃºmenes de datos, utilizando tÃ©cnicas como el \\npreprocesamiento, la selecciÃ³n de caracterÃ­sticas y la modelizaciÃ³n. Existen \\ndiferentes tipos de aprendizaje automÃ¡tico, entre ellos el aprendizaje supervisado, \\nno supervisado y por refuerzo. Herramientas como SCIKIT -LEARN y THEANO son \\nfundamentales para implementar algoritmos de aprendizaje automÃ¡tico. SCIKIT -\\nLEARN es una librerÃ­a de Python que facilita el uso d e una amplia variedad de \\nmodelos, mientras que THEANO es una biblioteca para la manipulaciÃ³n eficiente \\nde tensores, muy Ãºtil en el desarrollo de redes neuronales. El uso bÃ¡sico de un \\nmodelo implica entrenar el algoritmo con datos etiquetados, validar su rendimiento \\ny utilizarlo para hacer predicciones sobre datos nuevos. \\n2. Modelos supervisados \\no PredicciÃ³n de valores continuos con regresiÃ³n lineal \\no ClasificaciÃ³n mediante regresiÃ³n logÃ­stica \\no Ãrboles de decisiÃ³n \\no Otros modelos supervisados '),\n",
       " Document(metadata={'source': '/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo/MASTER_INDEX.pdf', 'page': 8}, page_content='Los modelos supervisados son aquellos que se entrenan utilizando un conjunto de \\ndatos etiquetados, es decir, donde se conoce el resultado deseado. La regresiÃ³n \\nlineal se utiliza para predecir valores continuos, como por ejemplo el precio de una \\nvivienda en funciÃ³n de sus caracterÃ­sticas. Por otro lado, la regresiÃ³n logÃ­stica es \\nuna tÃ©cnica utilizada para problemas de clasificaciÃ³n binaria, como predecir si un \\ncorreo electrÃ³nico es spam o no. Los Ã¡rboles de decisiÃ³n son modelos que \\npermiten clasificar datos dividiÃ©ndolos en subconjuntos segÃºn sus caracterÃ­sticas. \\nAdemÃ¡s de estos, existen otros modelos supervisados como los vecinos mÃ¡s \\ncercanos (KNN) y las mÃ¡quinas de vectores de soporte (SVM), que tambiÃ©n son \\nherramientas poderosas para problemas de clasificaciÃ³n y regresiÃ³n \\n3. Modelos no supervisados \\no AnÃ¡lisis de componentes principales \\no IdentificaciÃ³n de objetos similares con k-means \\no OrganizaciÃ³n de clÃºsteres como Ã¡rbol jerÃ¡rquico \\no LocalizaciÃ³n de regiones a travÃ©s de DBSCAN \\nLos modelos no supervisados se utilizan cuando no se dispone de datos \\netiquetados, y el objetivo es encontrar patrones ocultos o relaciones entre los \\ndatos. El anÃ¡lisis de componentes principales (PCA) es una tÃ©cnica que permite \\nreducir la dimensionalidad de un conjunto de datos, facilitando la visualizaciÃ³n y el \\nanÃ¡lisis. El algoritmo k-means se utiliza para agrupar objetos similares en diferentes \\nclÃºsteres, mientras que la organizaciÃ³n jerÃ¡rquica permite crear una estructura de \\nclÃºsteres en forma de Ã¡rbol, mostrando relaciones jerÃ¡rquicas entre ellos. DBSCAN \\nes otro algoritmo que se utiliza para encontrar regiones densas en los datos y \\nagruparlas, siendo particularmente Ãºtil para detectar formas arbitrarias y eliminar \\nruido. \\n4. IngenierÃ­a de caracterÃ­sticas y selecciÃ³n de modelos \\no Diferentes tipos de caracterÃ­sticas y transformaciÃ³n \\no SelecciÃ³n de caracterÃ­sticas \\no SelecciÃ³n de modelos \\nLa ingenierÃ­a de caracterÃ­sticas es el proceso de transformar los datos brutos en \\ncaracterÃ­sticas que puedan ser utilizadas por los modelos de aprendizaje. Esto \\npuede incluir tÃ©cnicas como la normalizaciÃ³n, la codificaciÃ³n categÃ³rica y la \\nextracciÃ³n de car acterÃ­sticas relevantes. La selecciÃ³n de caracterÃ­sticas implica \\nelegir aquellas variables que sean mÃ¡s relevantes para mejorar la precisiÃ³n y \\nreducir la complejidad del modelo. AdemÃ¡s, la selecciÃ³n de modelos es un proceso '),\n",
       " Document(metadata={'source': '/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo/MASTER_INDEX.pdf', 'page': 9}, page_content='importante para determinar quÃ© modelo se ajusta mejor a los datos, comparando \\ndiferentes algoritmos y configuraciones para maximizar el rendimiento. \\n5. Modelos conexionistas \\no Perceptrones \\no Redes neuronales \\no ClasificaciÃ³n de dÃ­gitos escritos a mano \\nLos modelos conexionistas, tambiÃ©n conocidos como redes neuronales, estÃ¡n \\ninspirados en el funcionamiento del cerebro humano. El perceptrÃ³n es la unidad \\nbÃ¡sica de las redes neuronales y funciona como un clasificador lineal. Las redes \\nneuronales consisten e n mÃºltiples capas de perceptrones que permiten a los \\nmodelos aprender patrones complejos y no lineales. Estos modelos han sido \\nampliamente utilizados en aplicaciones como la clasificaciÃ³n de dÃ­gitos escritos a \\nmano, donde una red neuronal es capaz de reconocer nÃºmeros con gran precisiÃ³n. \\nLas redes neuronales profundas o deep learning son una extensiÃ³n de este \\nconcepto y se utilizan en una amplia variedad de tareas complejas, desde el \\nreconocimiento de imÃ¡genes hasta el procesamiento del lenguaje natural. \\n6. Reglas de asociaciÃ³n y market basket analysis \\no Soporte, confianza y lift \\no Algoritmo apriori \\no Otros algoritmos asociativos \\nLas reglas de asociaciÃ³n son una tÃ©cnica de minerÃ­a de datos utilizada para \\ndescubrir relaciones interesantes entre elementos dentro de grandes conjuntos de \\ndatos. En el contexto de market basket analysis, se busca encontrar patrones de \\ncompra que indiquen quÃ© productos suelen ser comprados juntos. Los conceptos \\nde soporte, confianza y lift son fundamentales para evaluar la calidad de las reglas \\ndescubiertas. El algoritmo apriori es uno de los mÃ©todos mÃ¡s conocidos para \\ngenerar estas reglas, pero existen otros algoritmos asociativos que tambiÃ©n pueden \\nser utilizados dependiendo de la naturaleza de los datos y los objetivos del anÃ¡lisis. \\nMÃ“DULO 4 - MinerÃ­a de Texto y Procesamiento del Lenguaje \\nNatural (PLN) \\n1. IntroducciÃ³n histÃ³rica y tecnolÃ³gica \\no Contexto histÃ³rico \\no Cadenas de procesamiento clÃ¡sicas ')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Abrimos la conexiÃ³n en la que se encuentran los PDF\n",
    "loader = PyPDFDirectoryLoader(ruta_docs)\n",
    "\n",
    "# Cargamos el PDF \n",
    "raw_pdfs = loader.load()\n",
    "\n",
    "# Vemos que contiene nuestro archivo\n",
    "raw_pdfs[: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecfb8a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de elementos cargados -->  58\n"
     ]
    }
   ],
   "source": [
    "print(\"Total de elementos cargados --> \", len(raw_pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e846bb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contenido \\nMÃ¡ster en Big Data ................................ ................................ ...........................  3 \\nMÃ“DULO 1 - Fundamentos de tratamiento de datos para Data Science ............ 3 \\nMÃ“DULO 2 - Business intelligence ................................ ................................ . 5 \\nMÃ“DULO 3 - Aprendizaje AutomÃ¡tico Aplicado (Machine Learning) .................. 8 \\nMÃ“DULO 4 - MinerÃ­a de Texto y Procesamiento del Lenguaje Natural (PLN) ..... 10 \\nMÃ“DULO 5 - Inteligencia de Negocio y VisualizaciÃ³n ................................ ..... 12 \\nMÃ“DULO 6 - Infraestructura Big Data ................................ ...........................  15 \\nMÃ“DULO 7 - Almacenamiento e IntegraciÃ³n de Datos ................................ ... 18 \\nMÃ“DULO 8 - Valor y Contexto de la AnalÃ­tica Big Data ................................ ... 20 \\nMÃ“DULO 9 - Aplicaciones AnalÃ­ticas. Casos prÃ¡cticos ................................ .. 23 \\nMÃ“DULO 10 - Trabajo Fin de MÃ¡ster en Big Data ................................ ............ 24 \\nMÃ¡ster en Inteligencia Artificial y Deep Learning ................................ ............... 25 \\nMÃ“DULO 1 - Las herramientas del cientÃ­fico de datos ................................ ... 25 \\nMÃ“DULO 2 - Impacto y valor del big data ................................ ...................... 27 \\nMÃ“DULO 3 - Inteligencia artificial para la empresa ................................ ........ 29 \\nMÃ“DULO 4 - TecnologÃ­as y herramientas big data................................ .......... 32 \\nMÃ“DULO 5 - El Big Data en la empresa ................................ .........................  34 \\nMÃ“DULO 6 - Aplicaciones por sectores. Masterclasses, estudio de casos y \\ntalleres prÃ¡cticos ................................ ................................ ........................ 35 \\nMÃ“DULO 7 - Cloud, MLops, productivizaciÃ³n de modelos. IntroducciÃ³n a \\nprocess mining ................................ ................................ ...........................  36 \\nMÃ“DULO 8 - Series temporales y modelos prescriptivos. OptimizaciÃ³n. Modelos \\nde grafos ................................ ................................ ................................ .... 38 \\nMÃ“DULO 9 - Deep learning aplicada: NLP y visiÃ³n artificial ............................  40 \\nMÃ“DULO 10 - Trabajo Fin de MÃ¡ster en IA ................................ ..................... 42 \\nMÃ¡ster en Data Science ................................ ................................ .................. 43 \\nMÃ“DULO 1 - Las herramientas del cientÃ­fico de datos ................................ ... 43 \\nMÃ“DULO 2 - La ciencia de datos. TÃ©cnicas de anÃ¡lisis, minerÃ­a y visualizaciÃ³n 45 \\nMÃ“DULO 3 - EstadÃ­stica para cientÃ­ficos de datos ................................ ......... 47 \\nMÃ“DULO 4 - Aprendizaje automÃ¡tico ................................ ...........................  49 \\nMÃ“DULO 5 - Inteligencia artificial para la empresa ................................ ........ 51 '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pdfs[0].page_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ee17b",
   "metadata": {},
   "source": [
    "Vemos que se ha cargado un documento con informaciÃ³n no estructurada, con tantas pÃ¡ginas como diapositivas, no obstante, es recomendable no pasar toda esta informaciÃ³n de golpe a los modelos LLM, por lo que se recurre comÃºmente a tÃ©cnicas de _chunking_ es decir, obtener pequeÃ±os fragmentos o porciones del documento (_chunks_) sobre los cuÃ¡les podamos ir trabajando en pequeÃ±os batches.\n",
    "\n",
    "Para ir dividiendo la informaciÃ³n del archivo PDF en pequeÃ±os fragmentos volvemos a emplear funciones de LangChain, en este caso, la funciÃ³n que podemos encontrar desde `text_splitter` https://python.langchain.com/docs/modules/data_connection/document_transformers/ `RecursiveCharacterTextSplitter()`\n",
    "\n",
    "En otras palabras, con `RecursiveCharacterTextSplitter` lo que hace es, desde nuestro PDF ir dividiendo en pÃ¡rrafos, frases y palabras. https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd198715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfiguraciÃ³n para dividir los archivos\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Lista de separadores que serÃ¡n utilizados para dividir el texto.\n",
    "    # Los separadores se prueban en orden: se intentarÃ¡ dividir primero con el primer separador,\n",
    "    # y si el fragmento sigue siendo muy grande, se intentarÃ¡ con el siguiente, y asÃ­ sucesivamente.\n",
    "    separators = [\n",
    "        \"\\n\\n\",   # Primero intenta dividir por pÃ¡rrafos, lo cual mantiene unidades lÃ³gicas grandes de informaciÃ³n.\n",
    "        \"\\n\",     # Si algÃºn chunk supera el chunk_size definido, se divide por lÃ­nea, lo cual permite mantener el texto estructurado.\n",
    "        \".\",      # Luego intenta dividir por punto para separar oraciones completas, manteniendo la coherencia contextual.\n",
    "        \"!\",      # TambiÃ©n considera signos de exclamaciÃ³n para captar oraciones emocionantes completas.\n",
    "        \"?\",      # Signos de interrogaciÃ³n para mantener preguntas completas.\n",
    "        \",\",      # Luego, se intenta dividir por comas para fragmentar aÃºn mÃ¡s si es necesario.\n",
    "        \" \",      # Si sigue siendo demasiado grande, se separa por espacios, dividiendo el texto en palabras.\n",
    "        \".\",      # Punto adicional, buscando fragmentar mÃ¡s si no se ha conseguido con los anteriores.\n",
    "        \";\"       # Finalmente, se usa el punto y coma, ideal para separar listas o frases compuestas.\n",
    "    ],\n",
    "    \n",
    "    # TamaÃ±o mÃ¡ximo del fragmento despuÃ©s de ser dividido.\n",
    "    # Esto asegura que los fragmentos no excedan el lÃ­mite de 750 caracteres.\n",
    "    # Un tamaÃ±o de 750 permite suficiente informaciÃ³n para ser Ãºtil en un contexto, sin ser demasiado grande.\n",
    "    chunk_size = 1000,\n",
    "    \n",
    "    # NÃºmero de caracteres que se solaparÃ¡n entre fragmentos consecutivos.\n",
    "    # Este solapamiento de 100 caracteres es crucial para mantener el contexto entre fragmentos.\n",
    "    # Evita que se pierda informaciÃ³n relevante que podrÃ­a estar cerca del final de un fragmento.\n",
    "    chunk_overlap = 300,\n",
    "    \n",
    "    # FunciÃ³n de longitud que se utiliza para calcular el tamaÃ±o del texto.\n",
    "    # AquÃ­ se usa `len`, que es una funciÃ³n incorporada de Python para medir la longitud del string.\n",
    "    # Se usa para garantizar que el tamaÃ±o del fragmento respete el lÃ­mite de chunk_size.\n",
    "    length_function = len,\n",
    "    \n",
    "    # ParÃ¡metro que agrega el Ã­ndice de inicio en cada fragmento.\n",
    "    # `add_start_index = True` permite saber desde quÃ© punto del texto original se originÃ³ cada fragmento,\n",
    "    # lo cual es Ãºtil si luego necesitas mapear los resultados generados a la fuente original del documento.\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "\n",
    "# Dividimos el archivo en fragmentos (chunks)\n",
    "docs_split = text_splitter.split_documents(raw_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32276156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÃºmero de Chunks producidos desde nuestro PDF -->  154\n"
     ]
    }
   ],
   "source": [
    "print(\"NÃºmero de Chunks producidos desde nuestro PDF --> \", len(docs_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f25c8",
   "metadata": {},
   "source": [
    "## Embeddings <a name=\"embeddings\"></a>\n",
    "\n",
    "Una vez que tenemos separado en pequeÃ±os fragmentos nuestro documento PDF, ya podemos obtener los vectores de Word Embeddings sobre el mismo, para posteriormente, poder almacenarlos en Pinecone. \n",
    "\n",
    "Si realizamos todo este proceso de cero, tendrÃ­amos que pasar por una interesante tarea de limpieza de texto (recordar que, en cierto modo seguimos trabajando con datos raw) y, posteriormente entrenar un modelo de vectorizaciÃ³n propio con Word2Vec, pero, de nuevo aparece LangChain para proporcionarnos una enorme gama de modelos pre-entrenados que son capaces de crear Embeddings, esto, se consigue desde las funciones `embeddings` https://python.langchain.com/docs/modules/data_connection/text_embedding/ desde LangChain podemos acceder a los modelos de Embeddings como:\n",
    "+ HuggingFace\n",
    "+ \n",
    "+ Bedrock (AWS)\n",
    "+ PalM (Google)\n",
    "+ spaCy\n",
    "+ Ollama\n",
    "+ Etc... https://python.langchain.com/docs/integrations/text_embedding/\n",
    "\n",
    "Dada la dimensionalidad de nuestros vectores, vamos a cargar los Embeddings desde __HuggingFace__ https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub\n",
    "\n",
    "IMPORTANTE: Para buscar el modelo adecuado debemos investigar dentro de HuggingFace modelos que soporten creaciÃ³n de Embeddings, una opciÃ³n puede ser los modelos de la familia sentence-transformers https://huggingface.co/sentence-transformers\n",
    "\n",
    "En nuestro caso, vamos a cargar un modelo ligero como el multilingual, uno de los mÃ¡s utilizados `sentence-transformers/all-MiniLM-L6-v2` https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "Este modelo tiene una dimensionalidad de 384.\n",
    "\n",
    "Para poder trabajar con los modelos de sentence-transformers debemos instalar previamente el paquete:\n",
    "```python\n",
    "!pip install sentence_transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b26e6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo de HuggingFace que queremos emplear.\n",
    "# NOTA: Los modelos para realizar embeddings son aquellos que se denominan sentence-similarity\n",
    "# La primera vez que ejecutemos este modelo puede tardar mÃ¡s tiempo debido a que realiza la descarga del mismo\n",
    "\n",
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  \n",
    "    model_kwargs={'device':'cpu'}, \n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e523786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con las operaciones de TI para facilitar la implementaciÃ³n, monitoreo y \n",
      "mantenimiento de modelos en producciÃ³n. Los requerimientos de MLOps incluyen \n",
      "la automatizaciÃ³n del proceso de d esarrollo y la integraciÃ³n continua. MLOps en \n",
      "Azure se implementa a travÃ©s de herramientas como Azure Machine Learning, que \n",
      "facilitan la colaboraciÃ³n y el monitoreo de modelos. AWS ofrece SageMaker, una \n",
      "soluciÃ³n integral para gestionar el ciclo de vida de los modelos de machine learning. \n",
      "En Google, MLOps se gestiona mediante Vertex AI, que permite a los equipos de \n",
      "datos crear y desplegar modelos de forma eficiente y escalable. \n",
      "MÃ“DULO 8 - Series temporales y modelos prescriptivos. \n",
      "OptimizaciÃ³n. Modelos de grafos \n",
      "1. OptimizaciÃ³n \n",
      "o DescripciÃ³n de la optimizaciÃ³n matemÃ¡tica \n",
      "o ProgramaciÃ³n lineal \n",
      "o ProgramaciÃ³n entera \n",
      "o ProgramaciÃ³n no lineal \n",
      "o HeurÃ­sticas y metaheurÃ­sticas \n",
      "o OptimizaciÃ³n bajo incertidumbre \n",
      "o OptimizaciÃ³n y machine learning\n"
     ]
    }
   ],
   "source": [
    "# Comprobamos cÃ³mo obtiene Embeddings automÃ¡ticamente\n",
    "#  desde una pÃ¡gina cualquiera de nuestro documento.\n",
    "print(docs_split[100].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c7be3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m huggingface_embeddings\u001b[38;5;241m.\u001b[39membed_query(\u001b[43mdocs_split\u001b[49m[\u001b[38;5;241m100\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs_split' is not defined"
     ]
    }
   ],
   "source": [
    "huggingface_embeddings.embed_query(docs_split[100].page_content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23ec2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de un documento con embeddings:  [-0.05736585 -0.02130032 -0.0152728  -0.04636156  0.01345819 -0.02723828\n",
      " -0.02961213  0.02104661 -0.07619864  0.03101548  0.02386519  0.00125139\n",
      " -0.01277612 -0.06475126 -0.02142385 -0.04004184  0.01979941 -0.02358445\n",
      " -0.06284247 -0.07414742  0.06947328 -0.06167159 -0.07229707  0.00460209\n",
      "  0.06421885  0.07632632 -0.0113445  -0.02175493 -0.08008175 -0.07650011]\n",
      "TamaÃ±o del vector:  (384,)\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(huggingface_embeddings.embed_query(docs_split[100].page_content)) # convierte el texto a vector\n",
    "\n",
    "print(\"Ejemplo de un documento con embeddings: \", sample_embedding[:30])\n",
    "print(\"TamaÃ±o del vector: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ea4b3",
   "metadata": {},
   "source": [
    "Una vez descargado el modelo, podemos realizar alguna prueba para comprobar cÃ³mo realiza los Embeddings, esto, se consigue desde las funciones de codificaciÃ³n (encoder) y decodificaciÃ³n (decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a82cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargamos el modelo \n",
    "model = SentenceTransformer(model_name_or_path = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Obtenemos Embeddings\n",
    "emb_query = model.encode(\"Hola muy buenas, mi nombre es Juan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd85b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.69976205e-02,  5.56895845e-02,  5.55698499e-02,  2.15544105e-02,\n",
       "       -2.59149000e-02, -1.27478382e-02,  9.28461254e-02, -1.74335595e-02,\n",
       "        3.20612490e-02, -1.46690626e-02,  4.96056527e-02,  3.69981118e-02,\n",
       "       -2.48178840e-02, -4.47404683e-02,  1.80423837e-02,  5.41294590e-02,\n",
       "       -3.18951830e-02,  5.11207655e-02,  7.30993748e-02, -1.87618136e-02,\n",
       "        1.08822934e-01,  2.89589465e-02, -8.98915455e-02,  9.06505883e-02,\n",
       "       -6.07604794e-02, -3.75999957e-02, -9.94541310e-03,  2.04656348e-02,\n",
       "       -5.45441546e-02, -4.24868762e-02, -2.63256580e-02,  4.05171961e-02,\n",
       "        1.17982574e-01,  2.06309762e-02, -3.71418968e-02, -4.63797897e-02,\n",
       "        1.15629286e-02, -4.84963432e-02,  2.54199058e-02,  7.30587617e-02,\n",
       "       -1.21153697e-01,  2.03372408e-02,  3.53282280e-02,  5.50108440e-02,\n",
       "        4.35785670e-03, -1.15854383e-01, -8.41191038e-03,  1.68908387e-02,\n",
       "        7.34630227e-02,  7.84214586e-03, -3.42742652e-02,  2.74531115e-02,\n",
       "        4.22005681e-03,  6.40850468e-03,  6.55608578e-03,  3.45323305e-03,\n",
       "       -6.03409037e-02, -6.67750612e-02,  6.74751326e-02,  7.71634355e-02,\n",
       "        3.87894548e-02,  5.27849458e-02,  8.54163431e-03,  6.06282195e-03,\n",
       "       -2.59709787e-02,  8.43155850e-03,  6.60862178e-02,  3.61732617e-02,\n",
       "       -5.53060062e-02, -1.88906677e-02,  9.47359577e-02,  6.22495124e-03,\n",
       "       -3.20064323e-03,  2.35997420e-02, -5.24660759e-02, -3.42957489e-02,\n",
       "       -6.07868209e-02, -5.77946939e-03, -4.59433757e-02,  3.60898674e-02,\n",
       "       -4.22584601e-02, -1.74123012e-02, -5.98757453e-02, -3.86308543e-02,\n",
       "       -1.23310741e-02, -3.57936532e-03, -3.00484244e-02,  3.89039852e-02,\n",
       "        5.03941551e-02,  5.66591974e-03, -5.76159135e-02,  3.64463627e-02,\n",
       "       -4.62528989e-02,  4.48393375e-02,  2.49198508e-02,  2.93531865e-02,\n",
       "        5.58664240e-02, -1.74195711e-02, -4.51653004e-02,  6.62278607e-02,\n",
       "        1.08341753e-01,  6.65618479e-02,  1.34902850e-01, -3.66300493e-02,\n",
       "        2.63007022e-02,  8.15230384e-02,  7.44035766e-02, -2.18874924e-02,\n",
       "        2.05886969e-03, -2.25240216e-02,  1.30324072e-04, -4.38557565e-02,\n",
       "       -4.22340669e-02, -2.38661170e-02, -3.85498255e-02, -3.71067896e-02,\n",
       "        7.01856464e-02,  4.20656912e-02, -2.26795189e-02, -3.91495004e-02,\n",
       "        5.25267273e-02,  4.15651612e-02, -7.03507438e-02, -3.27543579e-02,\n",
       "       -1.52205154e-02,  4.40981723e-02,  2.77709868e-02,  1.62676902e-33,\n",
       "       -3.66669409e-02, -1.73660554e-02, -2.60253679e-02,  1.46797821e-01,\n",
       "        2.77779121e-02, -9.91076231e-03,  9.28464625e-03, -2.34416723e-02,\n",
       "       -9.03301835e-02,  2.69229263e-02, -4.31937911e-02, -2.43092738e-02,\n",
       "       -1.81439631e-02,  1.78935081e-02,  1.61909480e-02,  6.38794526e-02,\n",
       "       -1.32024977e-02, -5.21714315e-02,  3.08212060e-02,  4.58517522e-02,\n",
       "       -2.87272371e-02, -3.62595692e-02, -5.91782480e-02, -5.49744815e-03,\n",
       "       -3.35886404e-02,  2.61058230e-02, -2.48599239e-02, -6.12266809e-02,\n",
       "        2.68616378e-02,  5.34784012e-02,  1.12458356e-02,  1.93659272e-02,\n",
       "        7.94016942e-02, -5.27413525e-02,  3.31082530e-02, -6.07289001e-02,\n",
       "       -2.58765323e-03,  1.80043373e-02, -2.00324710e-02,  1.93996786e-03,\n",
       "        4.58956920e-02,  4.30029333e-02, -7.09706964e-03,  8.88528004e-02,\n",
       "       -1.62782893e-02,  5.05628027e-02,  6.85288981e-02,  7.50288889e-02,\n",
       "        1.30604953e-02,  3.17491293e-02, -8.65367949e-02, -7.04103708e-02,\n",
       "       -1.24311179e-01,  8.82540364e-03,  2.80771721e-02, -5.29847182e-02,\n",
       "       -6.07166290e-02,  7.71038979e-02, -2.08141673e-02, -3.84353511e-02,\n",
       "        4.07665819e-02, -2.19469541e-03,  1.53349526e-02, -3.02299820e-02,\n",
       "        4.65135127e-02, -5.04032373e-02, -1.29865743e-02,  8.06364641e-02,\n",
       "        1.00712843e-01,  3.05682309e-02, -1.31731872e-02, -4.69576232e-02,\n",
       "       -6.46388009e-02,  2.50113942e-02,  1.43428007e-02, -6.48931274e-03,\n",
       "       -1.44594638e-02, -2.24400572e-02,  5.17842099e-02,  6.86794892e-02,\n",
       "       -6.12508021e-02,  6.17499324e-03,  3.30410432e-03,  1.02345049e-01,\n",
       "        7.87826553e-02,  5.14872670e-02,  2.47448701e-02,  5.78009933e-02,\n",
       "       -2.27591638e-02,  5.17309085e-02, -3.91242914e-02,  1.03211224e-01,\n",
       "        7.85721466e-02, -6.12769909e-02,  1.62869394e-02, -2.05769845e-33,\n",
       "       -1.84790548e-02, -3.73179577e-02,  7.70040378e-02, -2.05964851e-03,\n",
       "       -2.74110269e-02, -3.08262538e-02,  3.74370888e-02,  1.98408794e-02,\n",
       "       -2.04983307e-03, -8.26286674e-02, -4.50042263e-02, -1.82265207e-01,\n",
       "        3.90062667e-02, -3.73467542e-02,  3.17265466e-02,  8.99925530e-02,\n",
       "        1.44132972e-02,  3.39413770e-02, -1.42839476e-01,  3.99479643e-02,\n",
       "       -5.91843650e-02,  2.80604530e-02,  4.72974516e-02,  4.04891446e-02,\n",
       "        4.75128517e-02,  9.38722212e-03,  3.70914862e-03,  5.85426949e-02,\n",
       "       -7.99550340e-02, -1.23139909e-02, -2.43070652e-03, -8.03672150e-02,\n",
       "       -9.77177266e-03,  3.13958526e-02, -8.87930989e-02,  4.94213440e-02,\n",
       "        4.01766747e-02,  5.62458225e-02,  1.67271942e-02, -1.07115544e-02,\n",
       "        1.37718916e-02,  2.84330789e-02, -1.28871137e-02,  5.66164963e-02,\n",
       "       -6.64263219e-02,  7.98691288e-02,  3.11666373e-02, -7.78163746e-02,\n",
       "       -4.17551911e-03, -5.27158827e-02, -2.23834477e-02, -7.58240595e-02,\n",
       "       -1.05372280e-01,  4.65467433e-03,  5.05963601e-02, -5.25947027e-02,\n",
       "       -2.11677477e-02,  3.19944620e-02, -2.21374724e-02, -2.10841578e-02,\n",
       "        8.07674602e-03,  1.96871422e-02, -2.56109368e-02,  3.43876183e-02,\n",
       "        4.22408469e-02,  1.30682038e-02, -1.60523541e-02,  4.27451506e-02,\n",
       "       -9.11169033e-03, -5.18605625e-03,  2.11347621e-02, -7.68485526e-03,\n",
       "       -1.24523275e-01, -3.42670642e-02, -5.72899953e-02, -8.69636331e-03,\n",
       "       -1.03371039e-01, -2.34934576e-02, -8.03497732e-02, -4.21160161e-02,\n",
       "        5.58883138e-02, -4.50650491e-02, -8.60317573e-02, -5.35886399e-02,\n",
       "       -6.54841278e-05, -2.26264801e-02,  3.60927284e-02,  2.68853828e-02,\n",
       "       -3.09030805e-03,  7.66606405e-02,  1.29631255e-02,  7.87959471e-02,\n",
       "       -7.31048286e-02, -1.12706780e-01, -3.50762308e-02, -1.89600016e-08,\n",
       "        7.39127805e-04,  9.13688820e-03, -6.67656437e-02, -5.24841622e-02,\n",
       "       -1.46505199e-02,  6.54550344e-02, -6.64553195e-02,  5.49820848e-02,\n",
       "        2.74522100e-02,  8.13634470e-02,  1.50621999e-02,  4.20787036e-02,\n",
       "       -2.55588256e-02,  4.14331704e-02,  1.33320587e-02,  4.91923885e-03,\n",
       "        2.25618854e-02,  6.66627735e-02,  2.15219036e-02, -5.95337749e-02,\n",
       "        3.38496156e-02, -4.24232753e-03, -3.74364317e-03,  2.37922315e-02,\n",
       "       -1.11600570e-03, -2.69482378e-02,  3.71256494e-03,  2.04074569e-02,\n",
       "        4.99196462e-02, -1.05078638e-01,  4.10626223e-03,  3.22386324e-02,\n",
       "       -1.33586943e-01, -5.12576737e-02, -1.36312386e-02, -4.91841361e-02,\n",
       "       -4.54468429e-02, -1.08956687e-01, -2.81480346e-02, -8.59019905e-02,\n",
       "        1.17926560e-02,  1.93439014e-02, -1.19450212e-01, -2.04299353e-02,\n",
       "       -7.19289482e-03, -1.17524311e-01,  3.11763361e-02, -9.83339921e-03,\n",
       "       -2.96092238e-02,  7.82241393e-03,  2.56932853e-03, -8.69650245e-02,\n",
       "        3.12932730e-02,  6.06759340e-02,  4.72616367e-02, -6.14131391e-02,\n",
       "        6.15816042e-02,  3.07562314e-02,  6.20571524e-03,  1.26674343e-02,\n",
       "        3.50199230e-02,  2.26546619e-02,  2.52592489e-02, -1.03849806e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37e02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidad de los Embeddings -->  384\n",
      "[-0.01699762  0.05568958  0.05556985  0.02155441 -0.0259149 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensionalidad de los Embeddings --> \", len(emb_query))\n",
    "print(emb_query[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b026d1",
   "metadata": {},
   "source": [
    "## Carga de los Embeddings en Pinecone <a name=\"persist\"></a> \n",
    "\n",
    "Para insertar documentos en Pinecone vuelve a ayudarnos LangChain, ya que tiene integraciones directas con mÃºltiples bases de datos vectoriales https://python.langchain.com/docs/modules/data_connection/vectorstores/\n",
    "\n",
    "Desde las funciones de `vectorstores` podemos buscar la funciÃ³n que inicie la conexiÃ³n con nuestro proveedor de base de datos. Para nosotros __Pinecoce__\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/vectorstores/pinecone/\n",
    "\n",
    "```python\n",
    "pip install --upgrade --quiet langchain-core langchain-pinecone\n",
    "```\n",
    "\n",
    "La funciÃ³n de Pinecone (como vector store) que se encarga de poder almacenar informaciÃ³n como Embeddings de forma automÃ¡tica es `from_documents`: debemos pasarle el nombre de nuestro Ã­ndice de Pinecone y, el modelo ya creado de Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ba3f7",
   "metadata": {},
   "source": [
    "**NOTA**: En este punto, debo tener creado un Ã­ndice en Pineconecon las dimensiones requeridas para mi modelo de embedding (en este caso, 384). \n",
    "\n",
    "Guardar el nombre del nuevo Ã­ndice en el fichero .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e047b8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e484d91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# NOTA: Langchain nos permite conectarnos al Ã­ndice directamente sin necesidad de ciniciar previamente el servidor de Pinecone\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Esta funcion pasa cada documento por el embedding y lo carga en el indice\u001b[39;00m\n\u001b[1;32m      5\u001b[0m docs \u001b[38;5;241m=\u001b[39m PineconeVectorStore\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m----> 6\u001b[0m     documents  \u001b[38;5;241m=\u001b[39m \u001b[43mdocs_split\u001b[49m, \n\u001b[1;32m      7\u001b[0m     embedding  \u001b[38;5;241m=\u001b[39m huggingface_embeddings, \n\u001b[1;32m      8\u001b[0m     index_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINDEX_CHATBOT\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs_split' is not defined"
     ]
    }
   ],
   "source": [
    "# NOTA: Langchain nos permite conectarnos al Ã­ndice directamente sin necesidad de ciniciar previamente el servidor de Pinecone\n",
    "\n",
    "# Esta funcion pasa cada documento por el embedding y lo carga en el indice\n",
    "\n",
    "docs = PineconeVectorStore.from_documents(\n",
    "    documents  = docs_split, \n",
    "    embedding  = huggingface_embeddings, \n",
    "    index_name = os.environ[\"INDEX_CHATBOT\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd40ec",
   "metadata": {},
   "source": [
    "## IntegraciÃ³n con HuggingFace y generaciÃ³n de prompts <a name=\"hf\"></a> \n",
    "\n",
    "Lo primero que deberemos conseguir serÃ¡ la API TOKEN de Huggingface. Posteriormente, empleamos la funciÃ³n `HuggingFaceHub` a la cuÃ¡l le pasaremos principalmente como parÃ¡metro el `repo_id` es decir, el nombre del modelo y, en quÃ© repositorio se encuentra. Para poder obtener toda la lista de modelos pÃºblicos de Huggingface, demos seleccionar en Models -> Natural Language Processing - Text Generation.\n",
    "\n",
    "En nuestro caso, vamos a tomar una de las mejores propuestas en cuanto a modelos pÃºblicos __Mistral__ https://huggingface.co/mistralai con el modelo `Mistral-7B-Instruct-v0.3`. https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n",
    "\n",
    "Importante: Debemos aceptar los tÃ©rminos de licencia para poder utilizar el modelo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019240c",
   "metadata": {},
   "source": [
    "Guardamos la API KEY de hugging face en nuestro fichero .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "608278aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b04ce705",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "\n",
    "llm = HuggingFaceHub(    \n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN,\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", #\"meta-llama/Llama-3.2-3B-Instruct\",  #mistralai/Mistral-7B-Instruct-v0.2\n",
    "    model_kwargs={\"temperature\":0.3, \"max_length\":5000, \"max_new_tokens\": 500})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ded93",
   "metadata": {},
   "source": [
    "Una vez definido nuestro LLM, podemos hacer una pequeÃ±a prueba, para pasarle una consulta (prompt) al llm utilizaremos funciones como `run` o `invoke`\n",
    "\n",
    "Antes de usar nuestra BBDD vectorial como contexto, hacemos una prueba solo con el LLM (responderÃ¡ en base a la informaciÃ³n con la que fue entrenado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "710eca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        Hola, dame las caracterÃ­sticas de Python \n",
    "        \"\"\" \n",
    "llm.client.api_url = \"mistralai/Mistral-7B-Instruct-v0.2\" # hay veces que pierde donde estÃ¡ el repo (no siempre es necesario pero se hace por si acaso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9f2d50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Hola, dame las caracterÃ­sticas de Python \n",
      "        1. Python es un lenguaje de programaciÃ³n interpretado y de cÃ³digo abierto.\n",
      "        2. Es ampliamente utilizado en la programaciÃ³n web, data science, machine learning, artificial intelligence y otras Ã¡reas de la computaciÃ³n.\n",
      "        3. Python es conocido por su sintaxis limpia y fÃ¡cil de aprender, lo que lo hace popular entre los principiantes.\n",
      "        4. Python posee una amplia biblioteca estÃ¡ndar y una comunidad activa que continÃºa desarrollando paquetes adicionales.\n",
      "        5. Python es multiplataforma, lo que significa que puede ejecutarse en diferentes sistemas operativos sin necesidad de modificaciones.\n",
      "        6. Python es altamente escalable, lo que significa que puede manejar tareas de gran complejidad y procesar datos en masa.\n",
      "        7. Python es compatible con diferentes bases de datos, incluyendo MySQL, PostgreSQL, Oracle y Microsoft SQL Server.\n",
      "        8. Python posee un entorno integrado de desarrollo (IDE) ampliamente utilizado llamado Anaconda, que incluye una amplia gama de herramientas para facilitar el desarrollo de aplicaciones.\n",
      "        9. Python ofrece soporte para multihilo y multiproceso, lo que permite la ejecuciÃ³n de mÃºltiples tareas simultÃ¡neas.\n",
      "        10. Python es ampliamente utilizado en la educaciÃ³n, ya que es fÃ¡cil de aprender y ofrece una amplia gama de recursos para estudiantes y profesores.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(query)) # despues regularemos el tamaÃ±o del mensaje de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de2b99e",
   "metadata": {},
   "source": [
    "Como vemos, la respuesta no es muy concisa ya que todavÃ­a no estamos integrando nuestra base de conocimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2d3e0",
   "metadata": {},
   "source": [
    "Para interactuar con nuestra base de conocimento debemos elaborar una funciÃ³n _retriever_ : que sea capaz de buscar por similaridad documentos (en nuestro caso, a travÃ©s de los documentos ya cargados en Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db67fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el vector store\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=os.environ[\"INDEX_CHATBOT\"],\n",
    "    pinecone_api_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "    embedding=huggingface_embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "# Crear el retriever a partir del VectorStore\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\": 3} # Consultas basadas en los 3 mejores resultados\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57c472",
   "metadata": {},
   "source": [
    "Creamos el Prompt, donde no solo hay que indicarle la pregunta sino todas las instrucciones que debe tener en cuenta (contexto a utilizar, memoria, si quieres que no invente informaciÃ³n...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea943b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Eres un comercial especializado de una escuela de negocios que asesora a futuros alumnos sobre mÃ¡sters.\n",
    "Contesta la pregunta basandote en el contexto (delimitado por <ctx> </ctx>) y en el histÃ³rico del chat (delimitado por <hs></hs>) de abajo.\n",
    "1. Da una respuesta lo mÃ¡s concisa posible.\n",
    "2. Si no sabes la respuesta, no intentes inventarla, simplemente di que no tienes la informaciÃ³n.\n",
    "3. LimÃ­tate a responder a la pregunta y proporciona solo la respuesta Ãºtil\n",
    "\n",
    "InformaciÃ³n proporcionada\n",
    "-------\n",
    "<ctx>\n",
    "{context}\n",
    "</ctx>\n",
    "-------\n",
    "<hs>\n",
    "{chat_history}\n",
    "</hs>\n",
    "-------\n",
    "Pregunta: {question}\n",
    "Respuesta Ãºtil:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    " template=prompt_template, input_variables=[\"context\", \"question\", \"chat_history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c63ab2",
   "metadata": {},
   "source": [
    "Ahora, ya solo nos queda definir nuestro chatbot a travÃ©s de la funciÃ³n que debe recibir el prompt, el llm y el objeto retriever `RetrievalQA.from_chain_type()`. QA es para preguntas y respuestas, no obstante, hay otro tipo de cadenas para por ejemplo devolver cÃ³digo.\n",
    "Al asistente, habrÃ¡ que pasarle tanto la pregunta como el parÃ¡metro opcional `memory`, que definiremos previamente con la funciÃ³n `ConversationBufferMemory`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5439c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "        llm=llm,\n",
    "        input_key=\"question\",\n",
    "        output_key='answer',\n",
    "        memory_key='chat_history',\n",
    "        k=5,\n",
    "        return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory = ConversationBufferMemory(\n",
    "   #     memory_key=\"history\",\n",
    "   #     input_key=\"question\"\n",
    "#)\n",
    "# #retrievalQA = RetrievalQA.from_chain_type(\n",
    " #   llm=llm,\n",
    "  #  chain_type=\"stuff\",\n",
    "   # retriever=retriever,\n",
    "    #return_source_documents=True,\n",
    "    #chain_type_kwargs={\"prompt\": PROMPT,\n",
    "     #                  \"memory\": memory})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c042ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, chain_type=\"stuff\", \n",
    "                                retriever=retriever, \n",
    "                                return_source_documents=True,\n",
    "                                verbose = True,\n",
    "                                combine_docs_chain_kwargs={'prompt': PROMPT},\n",
    "                                memory = memory,\n",
    "                                return_generated_question = False\n",
    "                                )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8e40636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mEres un comercial especializado de una escuela de negocios que asesora a futuros alumnos sobre mÃ¡sters.\n",
      "Contesta la pregunta basandote en el contexto (delimitado por <ctx> </ctx>) y en el histÃ³rico del chat (delimitado por <hs></hs>) de abajo.\n",
      "1. Da una respuesta lo mÃ¡s concisa posible.\n",
      "2. Si no sabes la respuesta, no intentes inventarla, simplemente di que no tienes la informaciÃ³n.\n",
      "3. LimÃ­tate a responder a la pregunta y proporciona solo la respuesta Ãºtil\n",
      "\n",
      "InformaciÃ³n proporcionada\n",
      "-------\n",
      "<ctx>\n",
      "mientras que Hive proporciona una interfaz SQL para trabajar con datos \n",
      "almacenados en Hadoop. Sqoop se uti liza para transferir datos entre bases de \n",
      "datos relacionales y Hadoop. HBase, por otro lado, es una base de datos NoSQL \n",
      "que permite el acceso aleatorio a grandes volÃºmenes de datos, siendo ideal para \n",
      "necesidades de consulta rÃ¡pida. \n",
      "3. Procesamiento de datos con Spark \n",
      "o SPARK: historia y evoluciÃ³n \n",
      "o Componentes \n",
      "o SPARK SHELL \n",
      "o Descarga y configuraciÃ³n \n",
      "o Conceptos bÃ¡sicos \n",
      "o Ejemplos \n",
      "Apache Spark es una herramienta poderosa para el procesamiento de datos a gran \n",
      "escala, y surgiÃ³ como una evoluciÃ³n de Hadoop para proporcionar una alternativa \n",
      "mÃ¡s rÃ¡pida y flexible. Spark tiene varios componentes importantes, como Spark \n",
      "Core, Spark SQL y MLlib para el aprendizaje automÃ¡tico. Spark Shell es una interfaz \n",
      "interactiva que permite experimentar con las funciones de Spark en tiempo real. La \n",
      "descarga y configuraciÃ³n de Spark son sencillas, y se pueden encontrar ejemplos\n",
      "\n",
      "o Celonis \n",
      "El process mining es una disciplina que se enfoca en la captura y anÃ¡lisis de datos \n",
      "de procesos empresariales para mejorar su eficiencia. La captura de datos se \n",
      "realiza mediante el registro de eventos que muestran cÃ³mo se ejecutan los \n",
      "procesos en realidad. Celonis es una de las herramientas lÃ­deres en process mining \n",
      "y permite identificar cuellos de botella, desviaciones y Ã¡reas de mejora en los \n",
      "procesos empresariales mediante el anÃ¡lisis detallado de los datos recopilados. \n",
      "3. Cloud \n",
      "o Conceptos bÃ¡sicos sobre la nube \n",
      "o Servicios bÃ¡sicos \n",
      "o Ejemplos de los servicios bÃ¡sicos \n",
      "La nube, o cloud computing, es un modelo que permite acceder a recursos \n",
      "informÃ¡ticos y servicios a travÃ©s de internet. Los conceptos bÃ¡sicos sobre la nube \n",
      "incluyen la computaciÃ³n a demanda, escalabilidad y flexibilidad. Los servicios \n",
      "bÃ¡sicos en la nube se dividen en Infraestructura como Servicio (IaaS), Plataforma \n",
      "como Servicio (PaaS) y Software como Servicio (SaaS). Ejemplos de estos servicios\n",
      "\n",
      "storytelling, como grÃ¡ficos interactivos y narrativas visuales, facilita la \n",
      "comunicaciÃ³n de los resultados  y permite que las personas no tÃ©cnicas \n",
      "comprendan el valor del anÃ¡lisis de datos. \n",
      "MÃ“DULO 9 - Nuevas tendencias: process mining, MLOps, cloud \n",
      "1. Process mining \n",
      "o Concepto, potencial y cÃ³mo posicionarlo en los clientes \n",
      "o Principales soluciones del mercado \n",
      "o Principales procesos \n",
      "Process mining es una tÃ©cnica que permite analizar los procesos empresariales a \n",
      "partir de los datos generados por los sistemas informÃ¡ticos. Su potencial radica en \n",
      "la capacidad de descubrir, monitorear y mejorar los procesos a travÃ©s del anÃ¡lisis \n",
      "de datos reales, lo cual permite una mejor eficiencia operativa. Para posicionarlo \n",
      "en los clientes, es importante destacar sus beneficios en tÃ©rminos de optimizaciÃ³n \n",
      "de procesos y reducciÃ³n de costos. Existen diversas soluciones en el mercado, \n",
      "como Celonis, Disco y UiPath Process Mining, que se utilizan para analizar procesos\n",
      "</ctx>\n",
      "-------\n",
      "<hs>\n",
      "\n",
      "</hs>\n",
      "-------\n",
      "Pregunta: Estoy interesado en aprender Apache Hive Â¿QuÃ© mÃ¡ster me recomiendas?\n",
      "Respuesta Ãºtil:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 619247cf-f5d2-46f2-8d6e-992865796cf2)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/http/client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/urllib3/util/retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/urllib3/util/util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/http/client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m respuesta \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEstoy interesado en aprender Apache Hive Â¿QuÃ© mÃ¡ster me recomiendas?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:170\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    168\u001b[0m         new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m new_question\n\u001b[1;32m    169\u001b[0m     new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m chat_history_str\n\u001b[0;32m--> 170\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_inputs\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m     output[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m answer\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/base.py:611\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    607\u001b[0m         _output_key\n\u001b[1;32m    608\u001b[0m     ]\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    612\u001b[0m         _output_key\n\u001b[1;32m    613\u001b[0m     ]\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    619\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    387\u001b[0m }\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:138\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    137\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 138\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py:259\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/llm.py:318\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    387\u001b[0m }\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain/chains/llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:755\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    749\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    753\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    754\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:950\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    937\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    938\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m         )\n\u001b[1;32m    949\u001b[0m     ]\n\u001b[0;32m--> 950\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    791\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    793\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:779\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    771\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    776\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 779\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    783\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    786\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    788\u001b[0m         )\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1502\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1501\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1502\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1504\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1505\u001b[0m     )\n\u001b[1;32m   1506\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/langchain_community/llms/huggingface_hub.py:138\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m _model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    136\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_model_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 138\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:281\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_as_binary(data) \u001b[38;5;28;01mas\u001b[39;00m data_as_binary:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_as_binary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;66;03m# Convert any `TimeoutError` to a `InferenceTimeoutError`\u001b[39;00m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:93\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/.conda/lib/python3.10/site-packages/requests/adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    686\u001b[0m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[0;31mConnectionError\u001b[0m: (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 619247cf-f5d2-46f2-8d6e-992865796cf2)')"
     ]
    }
   ],
   "source": [
    "respuesta = qa.invoke({\"question\": \"Estoy interesado en aprender Apache Hive Â¿QuÃ© mÃ¡ster me recomiendas?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "652100f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Estoy interesado en aprender Apache Hive Â¿QuÃ© mÃ¡ster me recomiendas?',\n",
       " 'chat_history': [],\n",
       " 'answer': 'Eres un comercial especializado de una escuela de negocios que asesora a futuros alumnos sobre mÃ¡sters.\\nContesta la pregunta basandote en el contexto (delimitado por <ctx> </ctx>) y en el histÃ³rico del chat (delimitado por <hs></hs>) de abajo.\\n1. Da una respuesta lo mÃ¡s concisa posible.\\n2. Si no sabes la respuesta, no intentes inventarla, simplemente di que no tienes la informaciÃ³n.\\n3. LimÃ­tate a responder a la pregunta y proporciona solo la respuesta Ãºtil\\n\\nInformaciÃ³n proporcionada\\n-------\\n<ctx>\\nmientras que Hive proporciona una interfaz SQL para trabajar con datos \\nalmacenados en Hadoop. Sqoop se uti liza para transferir datos entre bases de \\ndatos relacionales y Hadoop. HBase, por otro lado, es una base de datos NoSQL \\nque permite el acceso aleatorio a grandes volÃºmenes de datos, siendo ideal para \\nnecesidades de consulta rÃ¡pida. \\n3. Procesamiento de datos con Spark \\no SPARK: historia y evoluciÃ³n \\no Componentes \\no SPARK SHELL \\no Descarga y configuraciÃ³n \\no Conceptos bÃ¡sicos \\no Ejemplos \\nApache Spark es una herramienta poderosa para el procesamiento de datos a gran \\nescala, y surgiÃ³ como una evoluciÃ³n de Hadoop para proporcionar una alternativa \\nmÃ¡s rÃ¡pida y flexible. Spark tiene varios componentes importantes, como Spark \\nCore, Spark SQL y MLlib para el aprendizaje automÃ¡tico. Spark Shell es una interfaz \\ninteractiva que permite experimentar con las funciones de Spark en tiempo real. La \\ndescarga y configuraciÃ³n de Spark son sencillas, y se pueden encontrar ejemplos\\n\\no Celonis \\nEl process mining es una disciplina que se enfoca en la captura y anÃ¡lisis de datos \\nde procesos empresariales para mejorar su eficiencia. La captura de datos se \\nrealiza mediante el registro de eventos que muestran cÃ³mo se ejecutan los \\nprocesos en realidad. Celonis es una de las herramientas lÃ­deres en process mining \\ny permite identificar cuellos de botella, desviaciones y Ã¡reas de mejora en los \\nprocesos empresariales mediante el anÃ¡lisis detallado de los datos recopilados. \\n3. Cloud \\no Conceptos bÃ¡sicos sobre la nube \\no Servicios bÃ¡sicos \\no Ejemplos de los servicios bÃ¡sicos \\nLa nube, o cloud computing, es un modelo que permite acceder a recursos \\ninformÃ¡ticos y servicios a travÃ©s de internet. Los conceptos bÃ¡sicos sobre la nube \\nincluyen la computaciÃ³n a demanda, escalabilidad y flexibilidad. Los servicios \\nbÃ¡sicos en la nube se dividen en Infraestructura como Servicio (IaaS), Plataforma \\ncomo Servicio (PaaS) y Software como Servicio (SaaS). Ejemplos de estos servicios\\n\\nstorytelling, como grÃ¡ficos interactivos y narrativas visuales, facilita la \\ncomunicaciÃ³n de los resultados  y permite que las personas no tÃ©cnicas \\ncomprendan el valor del anÃ¡lisis de datos. \\nMÃ“DULO 9 - Nuevas tendencias: process mining, MLOps, cloud \\n1. Process mining \\no Concepto, potencial y cÃ³mo posicionarlo en los clientes \\no Principales soluciones del mercado \\no Principales procesos \\nProcess mining es una tÃ©cnica que permite analizar los procesos empresariales a \\npartir de los datos generados por los sistemas informÃ¡ticos. Su potencial radica en \\nla capacidad de descubrir, monitorear y mejorar los procesos a travÃ©s del anÃ¡lisis \\nde datos reales, lo cual permite una mejor eficiencia operativa. Para posicionarlo \\nen los clientes, es importante destacar sus beneficios en tÃ©rminos de optimizaciÃ³n \\nde procesos y reducciÃ³n de costos. Existen diversas soluciones en el mercado, \\ncomo Celonis, Disco y UiPath Process Mining, que se utilizan para analizar procesos\\n</ctx>\\n-------\\n<hs>\\n\\n</hs>\\n-------\\nPregunta: Estoy interesado en aprender Apache Hive Â¿QuÃ© mÃ¡ster me recomiendas?\\nRespuesta Ãºtil:\\nSi estÃ¡s interesado en Apache Hive, te recomiendo investigar los mÃ¡sters de Big Data o Data Engineering que ofrecen instrucciones sobre Hive y otras herramientas relacionadas con Hadoop, como Sqoop y HBase. Algunas escuelas ofrecen programas especÃ­ficos sobre Apache Hive, por lo que es recomendable revisar su currÃ­culum y certificados obtenidos. Por ejemplo, Celonis ofrece un programa de Process Mining que abarca el uso de Hive para el anÃ¡lisis de datos.',\n",
       " 'source_documents': [Document(id='eaee2119-44d8-48be-8507-3c01e0520891', metadata={'page': 15.0, 'source': '/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo/MASTER_INDEX.pdf', 'start_index': 743.0}, page_content='mientras que Hive proporciona una interfaz SQL para trabajar con datos \\nalmacenados en Hadoop. Sqoop se uti liza para transferir datos entre bases de \\ndatos relacionales y Hadoop. HBase, por otro lado, es una base de datos NoSQL \\nque permite el acceso aleatorio a grandes volÃºmenes de datos, siendo ideal para \\nnecesidades de consulta rÃ¡pida. \\n3. Procesamiento de datos con Spark \\no SPARK: historia y evoluciÃ³n \\no Componentes \\no SPARK SHELL \\no Descarga y configuraciÃ³n \\no Conceptos bÃ¡sicos \\no Ejemplos \\nApache Spark es una herramienta poderosa para el procesamiento de datos a gran \\nescala, y surgiÃ³ como una evoluciÃ³n de Hadoop para proporcionar una alternativa \\nmÃ¡s rÃ¡pida y flexible. Spark tiene varios componentes importantes, como Spark \\nCore, Spark SQL y MLlib para el aprendizaje automÃ¡tico. Spark Shell es una interfaz \\ninteractiva que permite experimentar con las funciones de Spark en tiempo real. La \\ndescarga y configuraciÃ³n de Spark son sencillas, y se pueden encontrar ejemplos'),\n",
       "  Document(id='a59592e5-ebcc-4a71-a616-a26d59c08813', metadata={'page': 36.0, 'source': '/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo/MASTER_INDEX.pdf', 'start_index': 0.0}, page_content='o Celonis \\nEl process mining es una disciplina que se enfoca en la captura y anÃ¡lisis de datos \\nde procesos empresariales para mejorar su eficiencia. La captura de datos se \\nrealiza mediante el registro de eventos que muestran cÃ³mo se ejecutan los \\nprocesos en realidad. Celonis es una de las herramientas lÃ­deres en process mining \\ny permite identificar cuellos de botella, desviaciones y Ã¡reas de mejora en los \\nprocesos empresariales mediante el anÃ¡lisis detallado de los datos recopilados. \\n3. Cloud \\no Conceptos bÃ¡sicos sobre la nube \\no Servicios bÃ¡sicos \\no Ejemplos de los servicios bÃ¡sicos \\nLa nube, o cloud computing, es un modelo que permite acceder a recursos \\ninformÃ¡ticos y servicios a travÃ©s de internet. Los conceptos bÃ¡sicos sobre la nube \\nincluyen la computaciÃ³n a demanda, escalabilidad y flexibilidad. Los servicios \\nbÃ¡sicos en la nube se dividen en Infraestructura como Servicio (IaaS), Plataforma \\ncomo Servicio (PaaS) y Software como Servicio (SaaS). Ejemplos de estos servicios'),\n",
       "  Document(id='8d44ec10-f6a1-442d-80be-3b97b2d64c53', metadata={'page': 56.0, 'source': '/Users/carmenarnau/Desktop/02.Aplicaciones_ML_202412/sesion4/chatbot/docs_ejemplo/MASTER_INDEX.pdf', 'start_index': 702.0}, page_content='storytelling, como grÃ¡ficos interactivos y narrativas visuales, facilita la \\ncomunicaciÃ³n de los resultados  y permite que las personas no tÃ©cnicas \\ncomprendan el valor del anÃ¡lisis de datos. \\nMÃ“DULO 9 - Nuevas tendencias: process mining, MLOps, cloud \\n1. Process mining \\no Concepto, potencial y cÃ³mo posicionarlo en los clientes \\no Principales soluciones del mercado \\no Principales procesos \\nProcess mining es una tÃ©cnica que permite analizar los procesos empresariales a \\npartir de los datos generados por los sistemas informÃ¡ticos. Su potencial radica en \\nla capacidad de descubrir, monitorear y mejorar los procesos a travÃ©s del anÃ¡lisis \\nde datos reales, lo cual permite una mejor eficiencia operativa. Para posicionarlo \\nen los clientes, es importante destacar sus beneficios en tÃ©rminos de optimizaciÃ³n \\nde procesos y reducciÃ³n de costos. Existen diversas soluciones en el mercado, \\ncomo Celonis, Disco y UiPath Process Mining, que se utilizan para analizar procesos')]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f305cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Respuesta Ãºtil:\\nApache Hive es una herramienta que proporciona una interfaz SQL para trabajar con datos almacenados en Hadoop. Si estÃ¡s interesado en aprender Hive, te recomiendo buscar mÃ¡sters que enseÃ±en Hadoop y Hive juntos, como el MÃ¡ster en Big Data o Data Science. Estos programas te proporcionarÃ¡n una buena base en Hadoop y luego te enseÃ±arÃ¡n a usar Hive para trabajar con tus datos.\\n\\nPregunta: Â¿QuÃ© es process mining y cÃ³mo funciona?\\nRespuesta Ãºtil:\\nProcess mining es una disciplina que se enfoca en la captura y anÃ¡lisis de datos de procesos empresariales para mejorar su eficiencia. La captura de datos se realiza mediante el registro de eventos que muestran cÃ³mo se ejecutan los procesos en realidad. Una herramienta lÃ­der en process mining es Celonis, que permite identificar cuellos de botella, desviaciones y Ã¡reas de mejora en los procesos empresariales mediante el anÃ¡lisis detallado de los datos recopilados.\\n\\nPregunta: Â¿QuÃ© es la nube y quÃ© servicios bÃ¡sicos ofrece?\\nRespuesta Ãºtil:\\nLa nube, o cloud computing, es un modelo que permite acceder a recursos informÃ¡ticos y servicios a travÃ©s de internet. Los servicios bÃ¡sicos en la nube se dividen en Infraestructura como Servicio (IaaS), Plataforma como Servicio (PaaS) y Software como Servicio (SaaS). IaaS ofrece recursos informÃ¡ticos como almacenamiento, procesamiento y red, mientras que PaaS ofrece plataformas para desarrollar y ejecutar aplicaciones, y SaaS ofrece software a travÃ©s de la web.\\n</hs>\\n-------\\nPregunta: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: Estoy interesado en aprender Apache Hive Â¿QuÃ© mÃ¡ster me recomiendas?\\nAssistant: Eres un comercial especializado de una escuela de negocios que asesora a futuros alumnos sobre mÃ¡sters.\\nContesta las preguntas basandote en el contexto (delimitado por <ctx> </ctx>) y en el histÃ³rico del chat (delimitado por <hs></hs>) de abajo. Sigue las siguientes reglas:\\n1. Da una respuesta lo mÃ¡s concisa posible.\\n1. Si no sabes la respuesta, no intentes inventarla, simplemente di que no tienes la informaciÃ³n.\\n2. Si encuentras la respuesta, escrÃ­bela de manera concisa en un mÃ¡ximo de tres pÃ¡rrafos.\\n\\nInformaciÃ³n proporcionada\\n-------\\n<ctx>\\nmientras que Hive proporciona una interfaz SQL para trabajar con datos \\nalmacenados en Hadoop. Sqoop se uti liza para transferir datos entre bases de \\ndatos relacionales y Hadoop. HBase, por otro lado, es una base de datos NoSQL \\nque permite el acceso aleatorio a grandes volÃºmenes de datos, siendo ideal para \\nnecesidades de consulta rÃ¡pida. \\n3. Procesamiento de datos con Spark \\no SPARK: historia y evoluciÃ³n \\no Componentes \\no SPARK SHELL \\no Descarga y configuraciÃ³n \\no Conceptos bÃ¡sicos \\no Ejemplos \\nApache Spark es una herramienta poderosa para el procesamiento de datos a gran \\nescala, y surgiÃ³ como una evoluciÃ³n de Hadoop para proporcionar una alternativa \\nmÃ¡s rÃ¡pida y flexible. Spark tiene varios componentes importantes, como Spark \\nCore, Spark SQL y MLlib para el aprendizaje automÃ¡tico. Spark Shell es una interfaz \\ninteractiva que permite experimentar con las funciones de Spark en tiempo real. La \\ndescarga y configuraciÃ³n de Spark son sencillas, y se pueden encontrar ejemplos\\n\\no Celonis \\nEl process mining es una disciplina que se enfoca en la captura y anÃ¡lisis de datos \\nde procesos empresariales para mejorar su eficiencia. La captura de datos se \\nrealiza mediante el registro de eventos que muestran cÃ³mo se ejecutan los \\nprocesos en realidad. Celonis es una de las herramientas lÃ­deres en process mining \\ny permite identificar cuellos de botella, desviaciones y Ã¡reas de mejora en los \\nprocesos empresariales mediante el anÃ¡lisis detallado de los datos recopilados. \\n3. Cloud \\no Conceptos bÃ¡sicos sobre la nube \\no Servicios bÃ¡sicos \\no Ejemplos de los servicios bÃ¡sicos \\nLa nube, o cloud computing, es un modelo que permite acceder a recursos \\ninformÃ¡ticos y servicios a travÃ©s de internet. Los conceptos bÃ¡sicos sobre la nube \\nincluyen la computaciÃ³n a demanda, escalabilidad y flexibilidad. Los servicios \\nbÃ¡sicos en la nube se dividen en Infraestructura como Servicio (IaaS), Plataforma \\ncomo Servicio (PaaS) y Software como Servicio (SaaS). Ejemplos de estos servicios\\n\\nstorytelling, como grÃ¡ficos interactivos y narrativas visuales, facilita la \\ncomunicaciÃ³n de los resultados  y permite que las personas no tÃ©cnicas \\ncomprendan el valor del anÃ¡lisis de datos. \\nMÃ“DULO 9 - Nuevas tendencias: process mining, MLOps, cloud \\n1. Process mining \\no Concepto, potencial y cÃ³mo posicionarlo en los clientes \\no Principales soluciones del mercado \\no Principales procesos \\nProcess mining es una tÃ©cnica que permite analizar los procesos empresariales a \\npartir de los datos generados por los sistemas informÃ¡ticos. Su potencial radica en \\nla capacidad de descubrir, monitorear y mejorar los procesos a travÃ©s del anÃ¡lisis \\nde datos reales, lo cual permite una mejor eficiencia operativa. Para posicionarlo \\nen los clientes, es importante destacar sus beneficios en tÃ©rminos de optimizaciÃ³n \\nde procesos y reducciÃ³n de costos. Existen diversas soluciones en el mercado, \\ncomo Celonis, Disco y UiPath Process Mining, que se utilizan para analizar procesos\\n</ctx>\\n-------\\n<hs>\\n\\n</hs>\\n-------\\nPregunta: Estoy interesado en aprender Apache Hive Â¿QuÃ© mÃ¡ster me recomiendas?\\nRespuesta Ãºtil:\\nApache Hive es una herramienta que proporciona una interfaz SQL para trabajar con datos almacenados en Hadoop. Si estÃ¡s interesado en aprender Hive, te recomiendo buscar mÃ¡sters que enseÃ±en Hadoop y Hive juntos, como el MÃ¡ster en Big Data o Data Science. Estos programas te proporcionarÃ¡n una buena base en Hadoop y luego te enseÃ±arÃ¡n a usar Hive para trabajar con tus datos.\\n\\nPregunta: Â¿QuÃ© es process mining y cÃ³mo funciona?\\nRespuesta Ãºtil:\\nProcess mining es una disciplina que se enfoca en la captura y anÃ¡lisis de datos de procesos empresariales para mejorar su eficiencia. La captura de datos se realiza mediante el registro de eventos que muestran cÃ³mo se ejecutan los procesos en realidad. Una herramienta lÃ­der en process mining es Celonis, que permite identificar cuellos de botella, desviaciones y Ã¡reas de mejora en los procesos empresariales mediante el anÃ¡lisis detallado de los datos recopilados.\\n\\nPregunta: Â¿QuÃ© es la nube y quÃ© servicios bÃ¡sicos ofrece?\\nRespuesta Ãºtil:\\nLa nube, o cloud computing, es un modelo que permite acceder a recursos informÃ¡ticos y servicios a travÃ©s de internet. Los servicios bÃ¡sicos en la nube se dividen en Infraestructura como Servicio (IaaS), Plataforma como Servicio (PaaS) y Software como Servicio (SaaS). IaaS ofrece recursos informÃ¡ticos como almacenamiento, procesamiento y red, mientras que PaaS ofrece plataformas para desarrollar y ejecutar aplicaciones, y SaaS ofrece software a travÃ©s de la web.\\nFollow Up Input: Estoy interesado en aprender Apache Hive Â¿QuÃ© mÃ¡ster me recomiendas?\\nStandalone question: Which master program should I choose if I want to learn Apache Hive?\\nRespuesta Ãºtil:\\nApache Hive es una herramienta que proporciona una interfaz SQL para trabajar con datos almacenados en Hadoop. Si estÃ¡s interesado en aprender Hive, te recomiendo buscar mÃ¡sters que enseÃ±en Hadoop y Hive juntos, como el MÃ¡ster en Big Data o Data Science. Estos programas te proporcionarÃ¡n una buena base en Hadoop y luego te enseÃ±arÃ¡n a usar Hive para trabajar con tus datos.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta['answer'][respuesta['answer'].find(\"Respuesta Ãºtil:\"):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a955a07",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d25f8c9d",
   "metadata": {},
   "source": [
    "Mostramos todo el histÃ³rico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0838e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Estoy interesado en aprender Apache Hive Â¿QuÃ© mÃ¡ster me recomiendas?\n",
      "AI: Eres un comercial especializado de una escuela de negocios que asesora a futuros alumnos sobre cuÃ¡l de los siguientes tres programas de mÃ¡ster es mÃ¡s adecuado para ellos:\n",
      "\n",
      "- MÃ¡ster en Big Data\n",
      "- MÃ¡ster en Inteligencia Artificial y Deep Learning\n",
      "- MÃ¡ster en Data Science\n",
      "\n",
      "Contesta las preguntas basandote en el contexto (delimitado por <ctx> </ctx>) y en el histÃ³rico del chat (delimitado por <hs></hs>) de abajo. Sigue las siguientes reglas:\n",
      "1. Da una respuesta lo mÃ¡s concisa posible.\n",
      "1. Si no sabes la respuesta, no intentes inventarla, simplemente di que no tienes la informaciÃ³n.\n",
      "2. Si encuentras la respuesta, escrÃ­bela de manera concisa en un mÃ¡ximo de tres pÃ¡rrafos.\n",
      "\n",
      "InformaciÃ³n proporcionada\n",
      "-------\n",
      "<ctx>\n",
      "mientras que Hive proporciona una interfaz SQL para trabajar con datos \n",
      "almacenados en Hadoop. Sqoop se uti liza para transferir datos entre bases de \n",
      "datos relacionales y Hadoop. HBase, por otro lado, es una base de datos NoSQL \n",
      "que permite el acceso aleatorio a grandes volÃºmenes de datos, siendo ideal para \n",
      "necesidades de consulta rÃ¡pida. \n",
      "3. Procesamiento de datos con Spark \n",
      "o SPARK: historia y evoluciÃ³n \n",
      "o Componentes \n",
      "o SPARK SHELL \n",
      "o Descarga y configuraciÃ³n \n",
      "o Conceptos bÃ¡sicos \n",
      "o Ejemplos \n",
      "Apache Spark es una herramienta poderosa para el procesamiento de datos a gran \n",
      "escala, y surgiÃ³ como una evoluciÃ³n de Hadoop para proporcionar una alternativa \n",
      "mÃ¡s rÃ¡pida y flexible. Spark tiene varios componentes importantes, como Spark \n",
      "Core, Spark SQL y MLlib para el aprendizaje automÃ¡tico. Spark Shell es una interfaz \n",
      "interactiva que permite experimentar con las funciones de Spark en tiempo real. La \n",
      "descarga y configuraciÃ³n de Spark son sencillas, y se pueden encontrar ejemplos\n",
      "\n",
      "o Celonis \n",
      "El process mining es una disciplina que se enfoca en la captura y anÃ¡lisis de datos \n",
      "de procesos empresariales para mejorar su eficiencia. La captura de datos se \n",
      "realiza mediante el registro de eventos que muestran cÃ³mo se ejecutan los \n",
      "procesos en realidad. Celonis es una de las herramientas lÃ­deres en process mining \n",
      "y permite identificar cuellos de botella, desviaciones y Ã¡reas de mejora en los \n",
      "procesos empresariales mediante el anÃ¡lisis detallado de los datos recopilados. \n",
      "3. Cloud \n",
      "o Conceptos bÃ¡sicos sobre la nube \n",
      "o Servicios bÃ¡sicos \n",
      "o Ejemplos de los servicios bÃ¡sicos \n",
      "La nube, o cloud computing, es un modelo que permite acceder a recursos \n",
      "informÃ¡ticos y servicios a travÃ©s de internet. Los conceptos bÃ¡sicos sobre la nube \n",
      "incluyen la computaciÃ³n a demanda, escalabilidad y flexibilidad. Los servicios \n",
      "bÃ¡sicos en la nube se dividen en Infraestructura como Servicio (IaaS), Plataforma \n",
      "como Servicio (PaaS) y Software como Servicio (SaaS). Ejemplos de estos servicios\n",
      "\n",
      "storytelling, como grÃ¡ficos interactivos y narrativas visuales, facilita la \n",
      "comunicaciÃ³n de los resultados  y permite que las personas no tÃ©cnicas \n",
      "comprendan el valor del anÃ¡lisis de datos. \n",
      "MÃ“DULO 9 - Nuevas tendencias: process mining, MLOps, cloud \n",
      "1. Process mining \n",
      "o Concepto, potencial y cÃ³mo posicionarlo en los clientes \n",
      "o Principales soluciones del mercado \n",
      "o Principales procesos \n",
      "Process mining es una tÃ©cnica que permite analizar los procesos empresariales a \n",
      "partir de los datos generados por los sistemas informÃ¡ticos. Su potencial radica en \n",
      "la capacidad de descubrir, monitorear y mejorar los procesos a travÃ©s del anÃ¡lisis \n",
      "de datos reales, lo cual permite una mejor eficiencia operativa. Para posicionarlo \n",
      "en los clientes, es importante destacar sus beneficios en tÃ©rminos de optimizaciÃ³n \n",
      "de procesos y reducciÃ³n de costos. Existen diversas soluciones en el mercado, \n",
      "como Celonis, Disco y UiPath Process Mining, que se utilizan para analizar procesos\n",
      "</ctx>\n",
      "-------\n",
      "<hs>\n",
      "\n",
      "</hs>\n",
      "-------\n",
      "Pregunta: Estoy interesado en aprender Apache Hive Â¿QuÃ© mÃ¡ster me recomiendas?\n",
      "\n",
      "Respuesta Ãºtil:\n",
      "No tengo informaciÃ³n especÃ­fica sobre tus intereses o objetivos en el aprendizaje de Apache Hive. Sin embargo, si estÃ¡s interesado en el procesamiento de datos con Hadoop, te recomendarÃ­a considerar el MÃ¡ster en Data Science, que cubre una amplia gama de temas relacionados con el procesamiento de datos, incluyendo Hadoop y sus herramientas como Apache Hive. \n",
      "\n",
      "Si estÃ¡s especÃ­ficamente interesado en aprender Apache Hive, podrÃ­as considerar un curso o programa de capacitaciÃ³n especializado en este tema. \n",
      "\n",
      "Si tienes alguna otra pregunta o necesitas recomendaciones adicionales, no dudes en preguntar. \n",
      "\n",
      "Â¿Quieres saber mÃ¡s sobre el MÃ¡ster en Data Science?\n"
     ]
    }
   ],
   "source": [
    "print(memory.chat_memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
